{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assessing Autism from Eye Movements Using Deep Learning: Professional Analysis Pipeline\n",
    "\n",
    "**Author:** Tal Alfi  \n",
    "**Advisor:** Prof. Ohad Ben-Shahar  \n",
    "**Institution:** Ben-Gurion University of the Negev  \n",
    "**Last Updated:** May 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Quick Start](#1-quick-start)\n",
    "2. [Configuration](#2-configuration)\n",
    "3. [Environment Setup](#3-environment-setup)\n",
    "4. [Data Overview](#4-data-overview)\n",
    "5. [Preprocessing Functions](#5-preprocessing-functions)\n",
    "6. [Visualization Functions](#6-visualization-functions)\n",
    "7. [Complete Pipeline](#7-complete-pipeline)\n",
    "8. [Batch Processing](#8-batch-processing)\n",
    "9. [Feature Extraction](#9-feature-extraction)\n",
    "10. [Export & Save](#10-export-save)\n",
    "11. [Common Issues & Solutions](#11-common-issues)\n",
    "12. [References & Resources](#12-references)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a professional eye-tracking data preprocessing pipeline for autism spectrum disorder (ASD) research. The pipeline processes raw eye-tracking data from ~450 children watching 90-second social videos, preparing it for machine learning classification and severity prediction tasks.\n",
    "\n",
    "### Key Features\n",
    "- ✅ Robust preprocessing with binocular validation\n",
    "- ✅ Automatic calibration correction\n",
    "- ✅ Blink detection and interpolation  \n",
    "- ✅ Outlier removal and smoothing\n",
    "- ✅ Professional visualization tools\n",
    "- ✅ Batch processing capabilities\n",
    "- ✅ Comprehensive error handling\n",
    "\n",
    "### Processing Flow\n",
    "```\n",
    "Raw Data → Validation → Calibration → Blink Handling → Outlier Removal → Clean Data\n",
    "```"
   ],
   "metadata": {},
   "id": "c51fe4085f7a731f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Quick Start\n",
    "\n",
    "Process an eye-tracking *.asc file using edf parser by Tal Alfi into a CSV file"
   ],
   "metadata": {},
   "id": "a6821457d59419b3"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Configuration\n\nAll preprocessing parameters and constants in one place:",
   "metadata": {},
   "id": "913a820334777b4e"
  },
  {
   "cell_type": "code",
   "source": "# === CONFIGURATION ===\n# Screen parameters\nSCREEN_WIDTH = 1280\nSCREEN_HEIGHT = 1024\nSCREEN_MARGIN = 500  # pixels outside screen to consider invalid\n\n# Recording parameters\nSAMPLING_RATE = 500  # Hz\nRECORDING_DURATION = 90  # seconds\nEXPECTED_SAMPLES = SAMPLING_RATE * RECORDING_DURATION\n\n# Preprocessing parameters\nPREPROCESSING_PARAMS = {\n    # Blink detection\n    'blink_gap_threshold': 5,      # samples to consider as gap\n    'blink_window': 24,            # max samples for natural blink (~48ms)\n    \n    # Pupil validation\n    'pupil_min': 200,              # minimum valid pupil size\n    'pupil_max': 1200,             # maximum valid pupil size\n    'pupil_smoothing_window': 250, # Gaussian smoothing window (500ms)\n    'pupil_adaptation_time': 1.0,  # seconds to exclude at start\n    \n    # Binocular disparity\n    'max_disparity': 150,          # maximum allowed disparity (pixels)\n    'disparity_percentile': 50,    # percentile for calibration correction\n    'velocity_threshold': 100,     # velocity threshold for stable periods\n    \n    # Missing data\n    'missing_data_threshold': 0.95,# threshold to remove columns\n    \n    # Visualization\n    'plot_sample_rate': 10,        # sample every N points for scatter plots\n    'figure_dpi': 100,             # figure resolution\n}\n\n# File paths\nDATA_DIR = 'files/'\nOUTPUT_DIR = 'output/'\nFIGURE_DIR = 'figures/'\n\n# Analysis parameters\nANALYSIS_PARAMS = {\n    'min_valid_data': 0.60,        # minimum 60% valid data required\n    'window_size': 500,            # 1-second windows for temporal analysis\n    'saccade_amplitude_bins': [0, 100, 500, 1000, 2000],  # pixels\n}\n\n# Visualization settings\nVIZ_PARAMS = {\n    'color_left': 'blue',\n    'color_right': 'red',\n    'alpha_scatter': 0.3,\n    'alpha_line': 0.7,\n    'figure_size': (12, 8),\n    'grid_alpha': 0.3,\n}\n\nprint(\"✅ Configuration loaded successfully\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:52.438350Z",
     "start_time": "2025-05-26T21:27:52.435510Z"
    }
   },
   "id": "db180b73477e8ae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Environment Setup\n\nImport all required libraries and set up the analysis environment:",
   "metadata": {},
   "id": "b3fce5c82156088b"
  },
  {
   "cell_type": "code",
   "source": "# Standard libraries\nimport os\nimport glob\nimport warnings\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional, Union\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle, Circle\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Scientific computing\nfrom scipy import stats, signal\nfrom scipy.spatial import distance\nfrom scipy.stats import entropy\nfrom scipy.ndimage import gaussian_filter1d\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n\n# Deep Learning (check availability)\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    DL_FRAMEWORK = 'pytorch'\n    print(\"✅ PyTorch available for deep learning\")\nexcept ImportError:\n    try:\n        import tensorflow as tf\n        from tensorflow import keras\n        DL_FRAMEWORK = 'tensorflow'\n        print(\"✅ TensorFlow available for deep learning\")\n    except ImportError:\n        DL_FRAMEWORK = None\n        print(\"⚠️ No deep learning framework found. Install PyTorch or TensorFlow.\")\n\n# Settings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# Set random seeds\nnp.random.seed(42)\nif DL_FRAMEWORK == 'pytorch':\n    torch.manual_seed(42)\nelif DL_FRAMEWORK == 'tensorflow':\n    tf.random.set_seed(42)\n\n# Create output directories if they don't exist\nfor directory in [OUTPUT_DIR, FIGURE_DIR]:\n    os.makedirs(directory, exist_ok=True)\n\nprint(f\"✅ Environment setup complete!\")\nprint(f\"📁 Data directory: {DATA_DIR}\")\nprint(f\"📁 Output directory: {OUTPUT_DIR}\")\nprint(f\"📁 Figure directory: {FIGURE_DIR}\")\nprint(f\"📊 Available CSV files: {len(glob.glob(os.path.join(DATA_DIR, '*.csv')))}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.110707Z",
     "start_time": "2025-05-26T21:27:52.570858Z"
    }
   },
   "id": "97436e9c7f8c0823",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch available for deep learning\n",
      "✅ Environment setup complete!\n",
      "📁 Data directory: files/\n",
      "📁 Output directory: output/\n",
      "📁 Figure directory: figures/\n",
      "📊 Available CSV files: 7\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Data Overview\n\nUnderstanding the eye-tracking data structure and format:",
   "metadata": {},
   "id": "8c46e5c3cb569221"
  },
  {
   "cell_type": "code",
   "source": "def explore_data_structure(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Load and explore the structure of an eye-tracking CSV file.\n    \n    Args:\n        file_path: Path to CSV file\n        \n    Returns:\n        pd.DataFrame: Loaded dataframe\n    \"\"\"\n    # Load data\n    df = pd.read_csv(file_path)\n    \n    # Extract subject ID from filename\n    subject_id = os.path.basename(file_path).split('_')[0]\n    \n    print(f\"📊 Data Structure Analysis\")\n    print(f\"{'='*50}\")\n    print(f\"Subject ID: {subject_id}\")\n    print(f\"Total samples: {len(df):,}\")\n    print(f\"Columns: {len(df.columns)}\")\n    \n    # Check recording duration\n    if 'timestamp' in df.columns:\n        duration_ms = df['timestamp'].max() - df['timestamp'].min()\n        duration_sec = duration_ms / 1000\n        print(f\"Recording duration: {duration_sec:.2f} seconds\")\n        print(f\"Effective sampling rate: {len(df) / duration_sec:.2f} Hz\")\n    \n    # Analyze missing data\n    missing_summary = df.isnull().sum()\n    missing_pct = (missing_summary / len(df) * 100).round(2)\n    \n    print(f\"\\n📋 Column Information:\")\n    print(f\"{'Column':<25} {'Type':<10} {'Missing %':<10} {'Description'}\")\n    print(f\"{'-'*70}\")\n    \n    column_descriptions = {\n        'timestamp': 'Recording timestamp (ms)',\n        'frame_number': 'Video frame number',\n        'x_left': 'Left eye X coordinate',\n        'y_left': 'Left eye Y coordinate',\n        'x_right': 'Right eye X coordinate',\n        'y_right': 'Right eye Y coordinate',\n        'pupil_left': 'Left pupil diameter',\n        'pupil_right': 'Right pupil diameter',\n        'is_fixation_left': 'Left eye fixation flag',\n        'is_fixation_right': 'Right eye fixation flag',\n        'is_saccade_left': 'Left eye saccade flag',\n        'is_saccade_right': 'Right eye saccade flag',\n        'is_blink_left': 'Left eye blink flag',\n        'is_blink_right': 'Right eye blink flag',\n    }\n    \n    for col in df.columns:\n        desc = column_descriptions.get(col, 'Additional metric')\n        print(f\"{col:<25} {str(df[col].dtype):<10} {missing_pct[col]:<10.1f} {desc}\")\n    \n    return df\n\n# Example usage\nif len(glob.glob(os.path.join(DATA_DIR, '*.csv'))) > 0:\n    sample_file = glob.glob(os.path.join(DATA_DIR, '*.csv'))[0]\n    df_sample = explore_data_structure(sample_file)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.278985Z",
     "start_time": "2025-05-26T21:27:54.214081Z"
    }
   },
   "id": "d3077be803b91e1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data Structure Analysis\n",
      "==================================================\n",
      "Subject ID: 1024539343\n",
      "Total samples: 45,010\n",
      "Columns: 24\n",
      "Recording duration: 90.02 seconds\n",
      "Effective sampling rate: 500.01 Hz\n",
      "\n",
      "📋 Column Information:\n",
      "Column                    Type       Missing %  Description\n",
      "----------------------------------------------------------------------\n",
      "timestamp                 int64      0.0        Recording timestamp (ms)\n",
      "frame_number              float64    0.0        Video frame number\n",
      "x_left                    float64    1.1        Left eye X coordinate\n",
      "y_left                    float64    1.1        Left eye Y coordinate\n",
      "pupil_left                float64    1.1        Left pupil diameter\n",
      "x_right                   float64    0.9        Right eye X coordinate\n",
      "y_right                   float64    0.9        Right eye Y coordinate\n",
      "pupil_right               float64    0.9        Right pupil diameter\n",
      "input                     float64    100.0      Additional metric\n",
      "cr_info                   object     99.5       Additional metric\n",
      "cr_left                   float64    0.0        Additional metric\n",
      "cr_right                  float64    0.0        Additional metric\n",
      "head_movement_left_x      float64    1.1        Additional metric\n",
      "head_movement_right_x     float64    0.9        Additional metric\n",
      "head_movement_magnitude   float64    1.7        Additional metric\n",
      "inter_pupil_distance      float64    1.7        Additional metric\n",
      "gaze_velocity_left        float64    1.1        Additional metric\n",
      "gaze_velocity_right       float64    0.9        Additional metric\n",
      "is_fixation_left          bool       0.0        Left eye fixation flag\n",
      "is_fixation_right         bool       0.0        Right eye fixation flag\n",
      "is_saccade_left           bool       0.0        Left eye saccade flag\n",
      "is_saccade_right          bool       0.0        Right eye saccade flag\n",
      "is_blink_left             bool       0.0        Left eye blink flag\n",
      "is_blink_right            bool       0.0        Right eye blink flag\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Preprocessing Functions\n\nAll preprocessing functions organized by category:",
   "metadata": {},
   "id": "fc2f913100ec46fc"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.1 Helper Functions",
   "metadata": {},
   "id": "b707775a98629659"
  },
  {
   "cell_type": "code",
   "source": "def add_helper_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add helper features for preprocessing and analysis.\n    \n    Args:\n        df: Input dataframe\n        \n    Returns:\n        df: DataFrame with added features\n    \"\"\"\n    df = df.copy()\n    \n    # Time in seconds from start\n    df['time_seconds'] = (df['timestamp'] - df['timestamp'].iloc[0]) / 1000.0\n    \n    # Velocity calculation\n    df['velocity_left'] = np.sqrt(\n        df['x_left'].diff()**2 + df['y_left'].diff()**2\n    ) * SAMPLING_RATE\n    \n    df['velocity_right'] = np.sqrt(\n        df['x_right'].diff()**2 + df['y_right'].diff()**2\n    ) * SAMPLING_RATE\n    \n    # Binocular disparity\n    df['disparity_x'] = df['x_left'] - df['x_right']\n    df['disparity_y'] = df['y_left'] - df['y_right']\n    df['disparity_total'] = np.sqrt(\n        df['disparity_x']**2 + df['disparity_y']**2\n    )\n    \n    # Validity flags\n    df['both_eyes_valid'] = (\n        df['x_left'].notna() & df['x_right'].notna() &\n        df['y_left'].notna() & df['y_right'].notna()\n    )\n    \n    # Screen bounds check\n    df['left_in_bounds'] = (\n        (df['x_left'] >= -20) & (df['x_left'] <= SCREEN_WIDTH + 20) &\n        (df['y_left'] >= -20) & (df['y_left'] <= SCREEN_HEIGHT + 20)\n    )\n    \n    df['right_in_bounds'] = (\n        (df['x_right'] >= -20) & (df['x_right'] <= SCREEN_WIDTH + 20) &\n        (df['y_right'] >= -20) & (df['y_right'] <= SCREEN_HEIGHT + 20)\n    )\n    \n    return df\n\n\ndef remove_empty_columns(df: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:\n    \"\"\"\n    Remove columns with more than threshold proportion of missing data.\n    \n    Args:\n        df: Input dataframe\n        threshold: Maximum allowed proportion of missing data\n        \n    Returns:\n        df: DataFrame with empty columns removed\n    \"\"\"\n    missing_proportion = df.isnull().sum() / len(df)\n    cols_to_remove = missing_proportion[missing_proportion > threshold].index.tolist()\n    \n    if cols_to_remove:\n        print(f\"Removing {len(cols_to_remove)} columns with >{threshold*100:.0f}% missing data:\")\n        for col in cols_to_remove:\n            print(f\"  - {col}: {missing_proportion[col]*100:.1f}% missing\")\n    \n    return df.drop(columns=cols_to_remove)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.299441Z",
     "start_time": "2025-05-26T21:27:54.294811Z"
    }
   },
   "id": "bace397efe096b35",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 Binocular Processing Functions",
   "metadata": {},
   "id": "bd342726037c76ac"
  },
  {
   "cell_type": "code",
   "source": "def enforce_binocular_validity(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Enforce binocular data validity: if one eye's data is invalid, \n    mark both eyes as invalid for that sample.\n    \n    Args:\n        df: Input dataframe\n        \n    Returns:\n        df: DataFrame with enforced binocular validity\n    \"\"\"\n    df = df.copy()\n    \n    # Find samples where only one eye has valid data\n    left_valid = df['x_left'].notna() & df['y_left'].notna()\n    right_valid = df['x_right'].notna() & df['y_right'].notna()\n    \n    # Identify monocular samples\n    monocular_samples = (left_valid & ~right_valid) | (~left_valid & right_valid)\n    \n    print(f\"Found {monocular_samples.sum()} monocular samples ({monocular_samples.sum()/len(df)*100:.1f}%)\")\n    \n    # Invalidate monocular samples\n    df.loc[monocular_samples, ['x_left', 'y_left', 'x_right', 'y_right']] = np.nan\n    df.loc[monocular_samples, ['pupil_left', 'pupil_right']] = np.nan\n    df.loc[monocular_samples, ['is_fixation_left', 'is_fixation_right']] = False\n    df.loc[monocular_samples, ['is_saccade_left', 'is_saccade_right']] = False\n    \n    # Update validity flag\n    df['both_eyes_valid'] = (\n        df['x_left'].notna() & df['x_right'].notna() &\n        df['y_left'].notna() & df['y_right'].notna()\n    )\n    \n    print(f\"After enforcement: {df['both_eyes_valid'].sum()} valid binocular samples ({df['both_eyes_valid'].sum()/len(df)*100:.1f}%)\")\n    \n    return df\n\n\ndef automatic_binocular_calibration(\n    df: pd.DataFrame,\n    velocity_threshold: float = None,\n    disparity_percentile: float = None,\n    verbose: bool = True\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    Automatically correct small alignment errors between left and right eyes.\n    \n    Args:\n        df: Input dataframe\n        velocity_threshold: Max velocity for stable periods (default from config)\n        disparity_percentile: Percentile for bias estimation (default from config)\n        verbose: Print calibration info\n        \n    Returns:\n        df: Calibrated dataframe\n        correction_info: Dictionary with calibration statistics\n    \"\"\"\n    df = df.copy()\n    \n    # Use defaults from config if not provided\n    if velocity_threshold is None:\n        velocity_threshold = PREPROCESSING_PARAMS['velocity_threshold']\n    if disparity_percentile is None:\n        disparity_percentile = PREPROCESSING_PARAMS['disparity_percentile']\n    \n    # Ensure velocities are calculated\n    if 'velocity_left' not in df.columns:\n        df['velocity_left'] = np.sqrt(\n            df['x_left'].diff()**2 + df['y_left'].diff()**2\n        ) * SAMPLING_RATE\n    \n    if 'velocity_right' not in df.columns:\n        df['velocity_right'] = np.sqrt(\n            df['x_right'].diff()**2 + df['y_right'].diff()**2\n        ) * SAMPLING_RATE\n    \n    # Find stable periods\n    stable_mask = (\n        (df['velocity_left'] < velocity_threshold) &\n        (df['velocity_right'] < velocity_threshold) &\n        df['both_eyes_valid']\n    )\n    \n    stable_samples = stable_mask.sum()\n    \n    if stable_samples < 100:  # Need minimum samples for calibration\n        if verbose:\n            print(\"⚠️ Insufficient stable samples for calibration\")\n        return df, {'improvement_percent': 0}\n    \n    # Calculate disparities during stable periods\n    stable_data = df[stable_mask]\n    x_disparity = stable_data['x_left'] - stable_data['x_right']\n    y_disparity = stable_data['y_left'] - stable_data['y_right']\n    \n    # Calculate robust bias estimates\n    x_bias = np.percentile(x_disparity, disparity_percentile)\n    y_bias = np.percentile(y_disparity, disparity_percentile)\n    \n    # Split correction between both eyes\n    x_correction_left = x_bias / 2\n    y_correction_left = y_bias / 2\n    x_correction_right = -x_bias / 2\n    y_correction_right = -y_bias / 2\n    \n    # Apply corrections\n    df['x_left'] = df['x_left'] - x_correction_left\n    df['y_left'] = df['y_left'] - y_correction_left\n    df['x_right'] = df['x_right'] - x_correction_right\n    df['y_right'] = df['y_right'] - y_correction_right\n    \n    # Recalculate disparities\n    df['disparity_x'] = df['x_left'] - df['x_right']\n    df['disparity_y'] = df['y_left'] - df['y_right']\n    df['disparity_total'] = np.sqrt(\n        df['disparity_x']**2 + df['disparity_y']**2\n    )\n    \n    # Calculate improvement\n    original_disparity = np.sqrt(x_disparity**2 + y_disparity**2).median()\n    corrected_disparity = df.loc[stable_mask, 'disparity_total'].median()\n    improvement = (1 - corrected_disparity / original_disparity) * 100\n    \n    correction_info = {\n        'x_bias': x_bias,\n        'y_bias': y_bias,\n        'stable_samples': stable_samples,\n        'original_disparity': original_disparity,\n        'corrected_disparity': corrected_disparity,\n        'improvement_percent': improvement,\n    }\n    \n    if verbose:\n        print(\"🔧 Automatic Binocular Calibration\")\n        print(f\"  Detected bias: ({x_bias:.1f}, {y_bias:.1f}) pixels\")\n        print(f\"  Based on {stable_samples:,} stable samples\")\n        print(f\"  Disparity reduced: {original_disparity:.1f} → {corrected_disparity:.1f} pixels\")\n        print(f\"  Improvement: {improvement:.1f}%\")\n    \n    return df, correction_info\n\n\ndef remove_extreme_disparities(\n    df: pd.DataFrame,\n    max_disparity: float = None\n) -> pd.DataFrame:\n    \"\"\"\n    Remove samples with physiologically impossible binocular disparities.\n    \n    Args:\n        df: Input dataframe\n        max_disparity: Maximum allowed disparity (default from config)\n        \n    Returns:\n        df: DataFrame with extreme disparities removed\n    \"\"\"\n    df = df.copy()\n    \n    if max_disparity is None:\n        max_disparity = PREPROCESSING_PARAMS['max_disparity']\n    \n    # Calculate disparity for valid samples\n    both_valid = df['both_eyes_valid']\n    \n    # Find extreme disparities\n    extreme_mask = both_valid & (df['disparity_total'] > max_disparity)\n    n_extreme = extreme_mask.sum()\n    \n    print(f\"Found {n_extreme} samples with extreme disparity (>{max_disparity} pixels)\")\n    print(f\"That's {n_extreme/both_valid.sum()*100:.2f}% of previously valid data\")\n    \n    # Invalidate extreme samples\n    df.loc[extreme_mask, ['x_left', 'y_left', 'x_right', 'y_right']] = np.nan\n    df.loc[extreme_mask, ['pupil_left', 'pupil_right']] = np.nan\n    df.loc[extreme_mask, 'both_eyes_valid'] = False\n    \n    # Also check for out-of-bounds data\n    out_of_bounds = both_valid & (\n        (df['x_left'] < -SCREEN_MARGIN) | (df['x_left'] > SCREEN_WIDTH + SCREEN_MARGIN) |\n        (df['y_left'] < -SCREEN_MARGIN) | (df['y_left'] > SCREEN_HEIGHT + SCREEN_MARGIN) |\n        (df['x_right'] < -SCREEN_MARGIN) | (df['x_right'] > SCREEN_WIDTH + SCREEN_MARGIN) |\n        (df['y_right'] < -SCREEN_MARGIN) | (df['y_right'] > SCREEN_HEIGHT + SCREEN_MARGIN)\n    )\n    n_oob = out_of_bounds.sum()\n    \n    if n_oob > 0:\n        print(f\"Found {n_oob} samples far outside screen bounds\")\n        df.loc[out_of_bounds, ['x_left', 'y_left', 'x_right', 'y_right']] = np.nan\n        df.loc[out_of_bounds, 'both_eyes_valid'] = False\n    \n    # Update validity\n    df['both_eyes_valid'] = (\n        df['x_left'].notna() & df['x_right'].notna() &\n        df['y_left'].notna() & df['y_right'].notna()\n    )\n    \n    print(f\"After filtering: {df['both_eyes_valid'].sum()} valid samples ({df['both_eyes_valid'].sum()/len(df)*100:.1f}%)\")\n    \n    return df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.334234Z",
     "start_time": "2025-05-26T21:27:54.325197Z"
    }
   },
   "id": "3a6b3e7fc59b105f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 Blink and Pupil Processing Functions",
   "metadata": {},
   "id": "97973f975d5d3022"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.380453Z",
     "start_time": "2025-05-26T21:27:54.370965Z"
    }
   },
   "cell_type": "code",
   "source": "def stabilize_bouncy_eye(\n    df: pd.DataFrame,\n    bounce_threshold: float = 5.0,\n    smoothing_window: int = 5,\n    verbose: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Stabilize a 'bouncy' eye that shows high-frequency noise compared to the other eye.\n    This is often due to calibration issues where one eye has more noise.\n\n    Args:\n        df: Input dataframe\n        bounce_threshold: Ratio of noise between eyes to trigger stabilization\n        smoothing_window: Window size for smoothing the bouncy eye\n        verbose: Print stabilization info\n\n    Returns:\n        df: DataFrame with stabilized eye data\n    \"\"\"\n    df = df.copy()\n\n    # Calculate high-frequency noise for each eye\n    # Use second derivative as a measure of \"bounciness\"\n    left_noise = df['x_left'].diff().diff().abs().mean() + df['y_left'].diff().diff().abs().mean()\n    right_noise = df['x_right'].diff().diff().abs().mean() + df['y_right'].diff().diff().abs().mean()\n\n    # Determine which eye is bouncy\n    if left_noise > bounce_threshold * right_noise:\n        bouncy_eye = 'left'\n        stable_eye = 'right'\n        noise_ratio = left_noise / right_noise\n    elif right_noise > bounce_threshold * left_noise:\n        bouncy_eye = 'right'\n        stable_eye = 'left'\n        noise_ratio = right_noise / left_noise\n    else:\n        if verbose:\n            print(f\"✓ No significant bounce detected (L/R ratio: {left_noise/right_noise:.2f})\")\n        return df\n\n    if verbose:\n        print(f\"🔧 Stabilizing {bouncy_eye} eye (noise ratio: {noise_ratio:.2f})\")\n\n    # Apply smoothing to the bouncy eye\n    for coord in ['x', 'y']:\n        col = f'{coord}_{bouncy_eye}'\n\n        # Only smooth valid data\n        valid_mask = df[col].notna()\n        if valid_mask.sum() > smoothing_window:\n            # Apply rolling median for robust smoothing\n            df.loc[valid_mask, col] = df.loc[valid_mask, col].rolling(\n                window=smoothing_window,\n                center=True,\n                min_periods=1\n            ).median()\n\n            # Apply additional Gaussian smoothing for very bouncy data\n            if noise_ratio > bounce_threshold * 2:\n                from scipy.ndimage import gaussian_filter1d\n                valid_data = df.loc[valid_mask, col].values\n                smoothed = gaussian_filter1d(valid_data, sigma=smoothing_window/4)\n                df.loc[valid_mask, col] = smoothed\n\n    # Recalculate noise after smoothing\n    if bouncy_eye == 'left':\n        new_noise = df['x_left'].diff().diff().abs().mean() + df['y_left'].diff().diff().abs().mean()\n    else:\n        new_noise = df['x_right'].diff().diff().abs().mean() + df['y_right'].diff().diff().abs().mean()\n\n    if verbose:\n        print(f\"  Noise reduced by {(1 - new_noise/noise_ratio)*100:.1f}%\")\n\n    return df\n\n\ndef enforce_gaze_quality_constraints(\n    df: pd.DataFrame,\n    max_gaze_disparity: float = 100,\n    verbose: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Enforce gaze quality constraints: remove samples with excessive binocular disparity.\n    For gaze data, we want both eyes looking at approximately the same screen location.\n\n    Args:\n        df: Input dataframe with gaze coordinates\n        max_gaze_disparity: Maximum allowed gaze disparity in pixels (default 100px)\n        verbose: Print constraint violations\n\n    Returns:\n        df: DataFrame with poor quality gaze samples removed\n    \"\"\"\n    df = df.copy()\n\n    # Find samples with excessive gaze disparity\n    valid_mask = df['both_eyes_valid']\n    \n    if valid_mask.sum() == 0:\n        if verbose:\n            print(\"⚠️ No valid binocular samples to check\")\n        return df\n    \n    # Calculate gaze disparity for valid samples\n    gaze_disparity = df.loc[valid_mask, 'disparity_total']\n    \n    # Find samples with excessive disparity\n    excessive_disparity = valid_mask & (df['disparity_total'] > max_gaze_disparity)\n    n_excessive = excessive_disparity.sum()\n    \n    if n_excessive > 0:\n        if verbose:\n            print(f\"⚠️ Found {n_excessive} samples with excessive gaze disparity (>{max_gaze_disparity}px)\")\n            print(f\"   ({n_excessive/valid_mask.sum()*100:.2f}% of valid data)\")\n            print(\"   → Removing poor quality gaze samples\")\n\n        # Invalidate excessive disparity samples\n        df.loc[excessive_disparity, ['x_left', 'y_left', 'x_right', 'y_right']] = np.nan\n        df.loc[excessive_disparity, 'both_eyes_valid'] = False\n\n    # Update validity\n    df['both_eyes_valid'] = (\n        df['x_left'].notna() & df['x_right'].notna() &\n        df['y_left'].notna() & df['y_right'].notna()\n    )\n    \n    final_valid = df['both_eyes_valid'].sum()\n    \n    if verbose:\n        # Show gaze disparity statistics\n        remaining_disparity = df.loc[df['both_eyes_valid'], 'disparity_total']\n        if len(remaining_disparity) > 0:\n            print(f\"\\n📊 Gaze disparity statistics (remaining data):\")\n            print(f\"   Mean: {remaining_disparity.mean():.1f}px\")\n            print(f\"   Median: {remaining_disparity.median():.1f}px\")\n            print(f\"   95th percentile: {remaining_disparity.quantile(0.95):.1f}px\")\n        \n        print(f\"\\n✓ After gaze quality filtering: {final_valid} valid samples\")\n\n    return df\n\n\ndef detect_and_fix_eye_swap(\n    df: pd.DataFrame,\n    check_window: int = 1000,\n    verbose: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Detect if eye gaze labels were swapped during recording and fix if needed.\n    For gaze data, we check if the \"right eye\" gaze is consistently to the left \n    of \"left eye\" gaze, which would indicate swapped labels.\n\n    Args:\n        df: Input dataframe\n        check_window: Number of samples to check for swap detection\n        verbose: Print swap detection info\n\n    Returns:\n        df: DataFrame with corrected eye labels\n    \"\"\"\n    df = df.copy()\n\n    # Check first valid window of data\n    valid_samples = df[df['both_eyes_valid']].head(check_window)\n\n    if len(valid_samples) < 100:\n        if verbose:\n            print(\"⚠️ Not enough valid samples to check for eye label swap\")\n        return df\n\n    # For gaze data, check if there's a systematic bias in gaze positions\n    # If eyes are consistently swapped, we might see the \"right eye\" gaze \n    # systematically to the left of \"left eye\" gaze\n    x_diff = valid_samples['x_right'] - valid_samples['x_left']\n    \n    # Calculate how often right eye gaze is to the right of left eye gaze\n    right_of_left_count = (x_diff > 0).sum()\n    right_ratio = right_of_left_count / len(valid_samples)\n\n    if verbose:\n        print(f\"✓ Gaze label check: {right_ratio*100:.1f}% of samples have right eye gaze to the right of left eye gaze\")\n        print(f\"   Mean horizontal gaze difference: {x_diff.mean():.1f}px\")\n\n    # Only swap if there's a very strong systematic bias (>80% reversed)\n    if right_ratio < 0.2:  # Less than 20% have correct orientation\n        if verbose:\n            print(f\"🔄 Systematic gaze label swap detected!\")\n            print(\"   → Swapping eye labels throughout recording\")\n\n        # Swap all eye-related columns\n        swap_columns = [\n            ('x_left', 'x_right'),\n            ('y_left', 'y_right'),\n            ('pupil_left', 'pupil_right'),\n            ('is_fixation_left', 'is_fixation_right'),\n            ('is_saccade_left', 'is_saccade_right'),\n            ('is_blink_left', 'is_blink_right'),\n            ('velocity_left', 'velocity_right'),\n        ]\n\n        for left_col, right_col in swap_columns:\n            if left_col in df.columns and right_col in df.columns:\n                df[left_col], df[right_col] = df[right_col].copy(), df[left_col].copy()\n\n        # Recalculate disparity\n        df['disparity_x'] = df['x_left'] - df['x_right']\n        df['disparity_y'] = df['y_left'] - df['y_right']\n        df['disparity_total'] = np.sqrt(df['disparity_x']**2 + df['disparity_y']**2)\n\n        if verbose:\n            print(\"   ✓ Eye labels corrected\")\n    \n    return df",
   "id": "757fc48567ad8726",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "def detect_and_handle_blinks(\n    df: pd.DataFrame,\n    gap_threshold: int = None,\n    blink_window: int = None,\n    verbose: bool = True\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    Detect and handle blinks in eye-tracking data.\n    Technical gaps (>threshold) are marked as NaN.\n    Natural blinks (≤window) are interpolated.\n    \n    Args:\n        df: Input dataframe\n        gap_threshold: Samples to consider as gap (default from config)\n        blink_window: Max samples for natural blink (default from config)\n        verbose: Print blink statistics\n        \n    Returns:\n        df: DataFrame with blinks handled\n        blink_stats: Dictionary with blink statistics\n    \"\"\"\n    df = df.copy()\n    \n    # Use defaults from config\n    if gap_threshold is None:\n        gap_threshold = PREPROCESSING_PARAMS['blink_gap_threshold']\n    if blink_window is None:\n        blink_window = PREPROCESSING_PARAMS['blink_window']\n    \n    # Initialize blink columns if not present\n    for col in ['is_blink_left', 'is_blink_right']:\n        if col not in df.columns:\n            df[col] = False\n    \n    # Function to find consecutive missing periods\n    def find_missing_periods(missing_mask):\n        periods = []\n        in_period = False\n        start_idx = 0\n        \n        for i, is_missing in enumerate(missing_mask):\n            if is_missing and not in_period:\n                start_idx = i\n                in_period = True\n            elif not is_missing and in_period:\n                periods.append((start_idx, i - 1))\n                in_period = False\n        \n        if in_period:\n            periods.append((start_idx, len(missing_mask) - 1))\n        \n        return periods\n    \n    # Process each eye\n    blink_stats = {}\n    \n    for eye in ['left', 'right']:\n        missing = df[f'x_{eye}'].isna() | df[f'y_{eye}'].isna()\n        periods = find_missing_periods(missing)\n        \n        technical_gaps = 0\n        natural_blinks = 0\n        interpolated_samples = 0\n        \n        for start, end in periods:\n            duration = end - start + 1\n            \n            if gap_threshold < duration <= blink_window:\n                # Natural blink - interpolate\n                if start > 0 and end < len(df) - 1:\n                    # Linear interpolation\n                    for col in [f'x_{eye}', f'y_{eye}']:\n                        df.loc[start:end, col] = np.interp(\n                            np.arange(start, end + 1),\n                            [start - 1, end + 1],\n                            [df.loc[start - 1, col], df.loc[end + 1, col]]\n                        )\n                    # Mark as blink\n                    df.loc[start:end, f'is_blink_{eye}'] = True\n                    natural_blinks += 1\n                    interpolated_samples += duration\n            elif duration > blink_window:\n                # Technical gap\n                technical_gaps += 1\n        \n        blink_stats[f'{eye}_eye'] = {\n            'technical_gaps': technical_gaps,\n            'natural_blinks': natural_blinks,\n            'interpolated_samples': interpolated_samples\n        }\n    \n    # Update validity\n    df['both_eyes_valid'] = (\n        df['x_left'].notna() & df['x_right'].notna() &\n        df['y_left'].notna() & df['y_right'].notna()\n    )\n    \n    if verbose:\n        print(\"👁️ Blink Detection and Handling\")\n        print(f\"  Left eye: {blink_stats['left_eye']['natural_blinks']} blinks, \"\n              f\"{blink_stats['left_eye']['technical_gaps']} gaps\")\n        print(f\"  Right eye: {blink_stats['right_eye']['natural_blinks']} blinks, \"\n              f\"{blink_stats['right_eye']['technical_gaps']} gaps\")\n    \n    return df, blink_stats\n\n\ndef preprocess_pupil_size(\n    df: pd.DataFrame,\n    min_pupil: float = None,\n    max_pupil: float = None,\n    window_size: int = None,\n    verbose: bool = True\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    Preprocess pupil size data by removing implausible values and smoothing.\n    \n    Args:\n        df: Input dataframe\n        min_pupil: Minimum valid pupil size (default from config)\n        max_pupil: Maximum valid pupil size (default from config)\n        window_size: Gaussian smoothing window (default from config)\n        verbose: Print pupil statistics\n        \n    Returns:\n        df: DataFrame with preprocessed pupil data\n        pupil_stats: Dictionary with pupil statistics\n    \"\"\"\n    df = df.copy()\n    \n    # Use defaults from config\n    if min_pupil is None:\n        min_pupil = PREPROCESSING_PARAMS['pupil_min']\n    if max_pupil is None:\n        max_pupil = PREPROCESSING_PARAMS['pupil_max']\n    if window_size is None:\n        window_size = PREPROCESSING_PARAMS['pupil_smoothing_window']\n    \n    adaptation_samples = int(PREPROCESSING_PARAMS['pupil_adaptation_time'] * SAMPLING_RATE)\n    \n    pupil_stats = {}\n    \n    for eye in ['left', 'right']:\n        col = f'pupil_{eye}'\n        if col not in df.columns:\n            continue\n        \n        original_data = df[col].copy()\n        \n        # Count implausible values\n        zero_pupils = (original_data == 0).sum()\n        too_small = ((original_data > 0) & (original_data < min_pupil)).sum()\n        too_large = (original_data > max_pupil).sum()\n        \n        # Remove implausible values\n        df.loc[(df[col] == 0) | (df[col] < min_pupil) | (df[col] > max_pupil), col] = np.nan\n        \n        # Exclude adaptation period\n        if len(df) > adaptation_samples:\n            df.loc[:adaptation_samples-1, col] = np.nan\n        \n        # Detect and remove sharp changes\n        if df[col].notna().sum() > 10:\n            pupil_diff = df[col].diff().abs()\n            threshold = pupil_diff.quantile(0.99)\n            sharp_changes = pupil_diff > threshold\n            sharp_change_count = sharp_changes.sum()\n            df.loc[sharp_changes, col] = np.nan\n        else:\n            sharp_change_count = 0\n        \n        # Apply Gaussian smoothing\n        valid_mask = df[col].notna()\n        if valid_mask.sum() > window_size:\n            valid_indices = np.where(valid_mask)[0]\n            valid_values = df.loc[valid_mask, col].values\n            \n            # Apply smoothing\n            sigma = window_size / 4\n            smoothed_values = gaussian_filter1d(valid_values, sigma=sigma, mode='nearest')\n            \n            # Put smoothed values back\n            df.loc[valid_indices, col] = smoothed_values\n        \n        # Calculate statistics\n        final_valid = df[col].notna().sum()\n        original_valid = original_data.notna().sum()\n        \n        pupil_stats[f'{eye}_eye'] = {\n            'original_valid': original_valid,\n            'zero_pupils': zero_pupils,\n            'too_small': too_small,\n            'too_large': too_large,\n            'sharp_changes': sharp_change_count,\n            'final_valid': final_valid,\n            'removed_total': original_valid - final_valid,\n            'removal_percentage': (original_valid - final_valid) / original_valid * 100 if original_valid > 0 else 0\n        }\n    \n    if verbose:\n        print(\"👁️ Pupil Size Preprocessing\")\n        for eye in ['left', 'right']:\n            if f'{eye}_eye' in pupil_stats:\n                stats = pupil_stats[f'{eye}_eye']\n                print(f\"  {eye.capitalize()} eye: {stats['removal_percentage']:.1f}% removed \"\n                      f\"({stats['final_valid']:,} valid samples)\")\n    \n    return df, pupil_stats\n\n\ndef round_coordinates_to_pixels(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n    \"\"\"\n    Round eye position values to whole numbers (integer pixels).\n    \n    Args:\n        df: Input dataframe\n        verbose: Print rounding info\n        \n    Returns:\n        df: DataFrame with rounded coordinates\n    \"\"\"\n    df = df.copy()\n    \n    coord_columns = ['x_left', 'y_left', 'x_right', 'y_right']\n    \n    # Round coordinates\n    for col in coord_columns:\n        if col in df.columns:\n            df[col] = df[col].round().astype('Int64')  # Nullable integer\n    \n    if verbose:\n        print(\"🎯 Coordinates rounded to integer pixels\")\n    \n    return df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.432524Z",
     "start_time": "2025-05-26T21:27:54.422636Z"
    }
   },
   "id": "ce1c5bdefd88fd73",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Visualization Functions\n\nReusable visualization components for eye-tracking data:",
   "metadata": {},
   "id": "ba3707121aaefb30"
  },
  {
   "cell_type": "code",
   "source": "def plot_before_after(\n    df_before: pd.DataFrame,\n    df_after: pd.DataFrame,\n    feature: str,\n    title: str = None,\n    figsize: Tuple[int, int] = None\n) -> plt.Figure:\n    \"\"\"\n    Generic before/after comparison plot for any feature.\n    \n    Args:\n        df_before: Original dataframe\n        df_after: Processed dataframe\n        feature: Feature column to plot\n        title: Plot title\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    if figsize is None:\n        figsize = VIZ_PARAMS['figure_size']\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    \n    # Before plot\n    if feature in df_before.columns:\n        ax1.plot(df_before['time_seconds'], df_before[feature],\n                 alpha=VIZ_PARAMS['alpha_line'], linewidth=0.5)\n        ax1.set_title(f'Before: {feature}')\n        ax1.set_xlabel('Time (seconds)')\n        ax1.set_ylabel(feature)\n        ax1.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    # After plot\n    if feature in df_after.columns:\n        ax2.plot(df_after['time_seconds'], df_after[feature],\n                 alpha=VIZ_PARAMS['alpha_line'], linewidth=0.5)\n        ax2.set_title(f'After: {feature}')\n        ax2.set_xlabel('Time (seconds)')\n        ax2.set_ylabel(feature)\n        ax2.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    if title:\n        fig.suptitle(title, fontsize=14)\n    \n    plt.tight_layout()\n    return fig\n\n\ndef plot_gaze_trajectory(\n    df: pd.DataFrame,\n    eye: str = 'both',\n    sample_rate: int = None,\n    ax: plt.Axes = None,\n    title: str = None\n) -> Union[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot gaze trajectory for one or both eyes.\n    \n    Args:\n        df: DataFrame with eye tracking data\n        eye: 'left', 'right', or 'both'\n        sample_rate: Sample every N points (default from config)\n        ax: Existing axis to plot on\n        title: Plot title\n        \n    Returns:\n        fig or ax: Figure if ax is None, otherwise axis\n    \"\"\"\n    if sample_rate is None:\n        sample_rate = PREPROCESSING_PARAMS['plot_sample_rate']\n    \n    if ax is None:\n        fig, ax = plt.subplots(figsize=VIZ_PARAMS['figure_size'])\n        return_fig = True\n    else:\n        return_fig = False\n    \n    # Sample data to avoid overplotting\n    sampled_df = df[::sample_rate]\n    \n    # Plot eye data\n    if eye in ['left', 'both']:\n        valid = sampled_df.dropna(subset=['x_left', 'y_left'])\n        ax.scatter(valid['x_left'], valid['y_left'],\n                   c=VIZ_PARAMS['color_left'], alpha=VIZ_PARAMS['alpha_scatter'],\n                   s=1, label='Left eye')\n    \n    if eye in ['right', 'both']:\n        valid = sampled_df.dropna(subset=['x_right', 'y_right'])\n        ax.scatter(valid['x_right'], valid['y_right'],\n                   c=VIZ_PARAMS['color_right'], alpha=VIZ_PARAMS['alpha_scatter'],\n                   s=1, label='Right eye')\n    \n    # Add screen boundaries\n    screen_rect = Rectangle((0, 0), SCREEN_WIDTH, SCREEN_HEIGHT,\n                            fill=False, edgecolor='black', linewidth=2)\n    ax.add_patch(screen_rect)\n    \n    # Set limits and labels\n    margin = 100\n    ax.set_xlim(-margin, SCREEN_WIDTH + margin)\n    ax.set_ylim(-margin, SCREEN_HEIGHT + margin)\n    ax.invert_yaxis()  # Invert Y axis for screen coordinates\n    ax.set_xlabel('X coordinate (pixels)')\n    ax.set_ylabel('Y coordinate (pixels)')\n    \n    if title:\n        ax.set_title(title)\n    else:\n        ax.set_title(f'Gaze Trajectory ({eye.capitalize()} eye)')\n    \n    ax.legend()\n    ax.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    ax.set_aspect('equal', adjustable='box')\n    \n    return fig if return_fig else ax\n\n\ndef plot_disparity_analysis(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (15, 10)\n) -> plt.Figure:\n    \"\"\"\n    Comprehensive binocular disparity analysis plot.\n    \n    Args:\n        df: DataFrame with disparity data\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=figsize)\n    \n    # 1. Disparity over time\n    ax = axes[0, 0]\n    ax.plot(df['time_seconds'], df['disparity_total'],\n            color='purple', alpha=0.7, linewidth=0.5)\n    ax.axhline(y=10, color='green', linestyle='--', label='Good (<10 px)')\n    ax.axhline(y=30, color='orange', linestyle='--', label='Acceptable (<30 px)')\n    ax.axhline(y=50, color='red', linestyle='--', label='Poor (>50 px)')\n    ax.set_ylabel('Binocular Disparity (pixels)')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_title('Disparity Throughout Recording')\n    ax.set_ylim(0, min(200, df['disparity_total'].quantile(0.99) * 1.1))\n    ax.legend()\n    ax.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    # 2. Disparity distribution\n    ax = axes[0, 1]\n    valid_disparity = df['disparity_total'].dropna()\n    ax.hist(valid_disparity, bins=50, color='purple', alpha=0.7, edgecolor='black')\n    ax.axvline(x=valid_disparity.median(), color='red', linestyle='--',\n               label=f'Median: {valid_disparity.median():.1f} px')\n    ax.axvline(x=valid_disparity.mean(), color='blue', linestyle='--',\n               label=f'Mean: {valid_disparity.mean():.1f} px')\n    ax.set_xlabel('Disparity (pixels)')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Disparity Distribution')\n    ax.legend()\n    ax.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    # 3. Disparity pattern (X vs Y)\n    ax = axes[1, 0]\n    sample_data = df[::PREPROCESSING_PARAMS['plot_sample_rate']].dropna(\n        subset=['disparity_x', 'disparity_y'])\n    scatter = ax.scatter(sample_data['disparity_x'], sample_data['disparity_y'],\n                         alpha=0.5, s=5, c=sample_data['time_seconds'], cmap='viridis')\n    \n    # Add reference circles\n    for radius, color in [(10, 'green'), (30, 'orange'), (50, 'red')]:\n        circle = Circle((0, 0), radius, fill=False, color=color,\n                        linestyle='--', label=f'{radius} px', alpha=0.7)\n        ax.add_patch(circle)\n    \n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.axvline(x=0, color='black', linewidth=0.5)\n    ax.set_xlabel('Horizontal Disparity (pixels)')\n    ax.set_ylabel('Vertical Disparity (pixels)')\n    ax.set_title('Binocular Disparity Pattern')\n    ax.set_aspect('equal', adjustable='box')\n    ax.legend(loc='upper right', fontsize='small')\n    ax.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    # 4. Validity over time\n    ax = axes[1, 1]\n    window_size = ANALYSIS_PARAMS['window_size']\n    validity_over_time = []\n    time_points = []\n    \n    for i in range(0, len(df) - window_size, window_size // 2):\n        window = df.iloc[i:i+window_size]\n        validity_percent = window['both_eyes_valid'].sum() / len(window) * 100\n        validity_over_time.append(validity_percent)\n        time_points.append(window['time_seconds'].mean())\n    \n    ax.plot(time_points, validity_over_time, 'g-', linewidth=2)\n    ax.axhline(y=60, color='orange', linestyle='--', label='60% threshold')\n    ax.axhline(y=80, color='green', linestyle='--', label='80% threshold')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Data Validity (%)')\n    ax.set_title('Data Validity Over Time (1-second windows)')\n    ax.set_ylim(0, 105)\n    ax.legend()\n    ax.grid(True, alpha=VIZ_PARAMS['grid_alpha'])\n    \n    plt.suptitle('Binocular Eye-Tracking Quality Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n\ndef plot_preprocessing_summary(\n    preprocessing_info: Dict,\n    save_path: str = None\n) -> plt.Figure:\n    \"\"\"\n    Create a summary plot of preprocessing statistics.\n    \n    Args:\n        preprocessing_info: Dictionary with preprocessing statistics\n        save_path: Optional path to save figure\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Data retention through pipeline\n    steps = ['Initial', 'Blinks', 'Binocular', 'Calibration', 'Disparity', 'Final']\n    values = [\n        preprocessing_info['initial_samples'],\n        preprocessing_info['steps'].get('after_blinks', preprocessing_info['initial_samples']),\n        preprocessing_info['steps']['after_binocular'],\n        preprocessing_info['steps']['after_calibration'],\n        preprocessing_info['steps']['after_disparity'],\n        preprocessing_info['final_valid_samples']\n    ]\n    \n    ax1.plot(steps, values, 'o-', linewidth=2, markersize=8)\n    ax1.set_ylabel('Valid Samples')\n    ax1.set_title('Data Retention Through Pipeline')\n    ax1.grid(True, alpha=0.3)\n    \n    # Add percentage labels\n    for i, (step, value) in enumerate(zip(steps, values)):\n        percent = value / preprocessing_info['initial_samples'] * 100\n        ax1.text(i, value + 500, f'{percent:.1f}%', ha='center', va='bottom')\n    \n    # Preprocessing impact breakdown\n    categories = ['Blinks\\nInterpolated', 'Monocular\\nRemoved', 'Extreme\\nDisparity', 'Other']\n    \n    blink_impact = (preprocessing_info['blink_stats']['left_eye']['interpolated_samples'] +\n                    preprocessing_info['blink_stats']['right_eye']['interpolated_samples']) / 2\n    \n    impacts = [\n        blink_impact,\n        preprocessing_info['initial_samples'] - preprocessing_info['steps']['after_binocular'],\n        preprocessing_info['steps']['after_calibration'] - preprocessing_info['steps']['after_disparity'],\n        preprocessing_info['initial_samples'] - preprocessing_info['final_valid_samples'] - \n        (preprocessing_info['initial_samples'] - preprocessing_info['steps']['after_binocular']) -\n        (preprocessing_info['steps']['after_calibration'] - preprocessing_info['steps']['after_disparity'])\n    ]\n    \n    # Ensure no negative values\n    impacts = [max(0, impact) for impact in impacts]\n    \n    ax2.bar(categories, impacts, color=['green', 'orange', 'red', 'gray'])\n    ax2.set_ylabel('Samples Affected')\n    ax2.set_title('Preprocessing Impact by Category')\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    # Overall title\n    retention_rate = preprocessing_info['retention_rate']\n    plt.suptitle(f'Preprocessing Summary - {retention_rate:.1f}% Data Retained', fontsize=14)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=PREPROCESSING_PARAMS['figure_dpi'], bbox_inches='tight')\n    \n    return fig",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.486728Z",
     "start_time": "2025-05-26T21:27:54.474691Z"
    }
   },
   "id": "8b26480820c8d1a9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "def plot_pupil_analysis(\n    df_before: pd.DataFrame,\n    df_after: pd.DataFrame,\n    figsize: Tuple[int, int] = (18, 14)\n) -> plt.Figure:\n    \"\"\"\n    Comprehensive pupil size analysis showing before/after preprocessing.\n    \n    Args:\n        df_before: Original dataframe\n        df_after: Preprocessed dataframe\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    fig, axes = plt.subplots(3, 2, figsize=figsize)\n    \n    # Row 1: Full time series\n    ax = axes[0, 0]\n    ax.plot(df_before['time_seconds'], df_before['pupil_left'], \n            color='blue', alpha=0.5, linewidth=0.5, label='Left pupil')\n    ax.plot(df_before['time_seconds'], df_before['pupil_right'], \n            color='red', alpha=0.5, linewidth=0.5, label='Right pupil')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Pupil Size (arbitrary units)')\n    ax.set_title('BEFORE: Raw Pupil Size Data')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    ax = axes[0, 1]\n    valid_left = df_after[df_after['pupil_left'].notna()]\n    valid_right = df_after[df_after['pupil_right'].notna()]\n    ax.plot(valid_left['time_seconds'], valid_left['pupil_left'], \n            color='blue', alpha=0.7, linewidth=1, label='Left pupil')\n    ax.plot(valid_right['time_seconds'], valid_right['pupil_right'], \n            color='red', alpha=0.7, linewidth=1, label='Right pupil')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Pupil Size (arbitrary units)')\n    ax.set_title('AFTER: Preprocessed Pupil Size (smoothed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Row 2: Zoomed view (10-20 seconds)\n    ax = axes[1, 0]\n    mask = (df_before['time_seconds'] >= 10) & (df_before['time_seconds'] <= 20)\n    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'pupil_left'], \n            'b-', alpha=0.7, linewidth=1, marker='o', markersize=2, label='Left pupil')\n    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'pupil_right'], \n            'r-', alpha=0.7, linewidth=1, marker='o', markersize=2, label='Right pupil')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Pupil Size')\n    ax.set_title('BEFORE: Raw Data (10-20s zoom)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    ax = axes[1, 1]\n    mask = (df_after['time_seconds'] >= 10) & (df_after['time_seconds'] <= 20)\n    valid_left = df_after[mask & df_after['pupil_left'].notna()]\n    valid_right = df_after[mask & df_after['pupil_right'].notna()]\n    ax.plot(valid_left['time_seconds'], valid_left['pupil_left'], \n            'b-', alpha=0.7, linewidth=2, label='Left pupil')\n    ax.plot(valid_right['time_seconds'], valid_right['pupil_right'], \n            'r-', alpha=0.7, linewidth=2, label='Right pupil')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Pupil Size')\n    ax.set_title('AFTER: Smoothed Data (10-20s zoom)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Row 3: Distributions\n    ax = axes[2, 0]\n    left_before = df_before['pupil_left'].dropna()\n    right_before = df_before['pupil_right'].dropna()\n    ax.hist(left_before, bins=50, alpha=0.5, color='blue', \n            label=f'Left (n={len(left_before)})', density=True)\n    ax.hist(right_before, bins=50, alpha=0.5, color='red', \n            label=f'Right (n={len(right_before)})', density=True)\n    ax.set_xlabel('Pupil Size')\n    ax.set_ylabel('Density')\n    ax.set_title('BEFORE: Pupil Size Distribution')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    ax = axes[2, 1]\n    left_after = df_after['pupil_left'].dropna()\n    right_after = df_after['pupil_right'].dropna()\n    ax.hist(left_after, bins=50, alpha=0.5, color='blue', \n            label=f'Left (n={len(left_after)})', density=True)\n    ax.hist(right_after, bins=50, alpha=0.5, color='red', \n            label=f'Right (n={len(right_after)})', density=True)\n    ax.set_xlabel('Pupil Size')\n    ax.set_ylabel('Density')\n    ax.set_title('AFTER: Pupil Size Distribution')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.suptitle('Pupil Size Data: Before vs After Preprocessing', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n\ndef plot_preprocessing_pipeline(\n    df_list: List[pd.DataFrame],\n    titles: List[str],\n    figsize: Tuple[int, int] = (20, 16)\n) -> plt.Figure:\n    \"\"\"\n    Visualize data through each preprocessing step.\n    \n    Args:\n        df_list: List of dataframes at each pipeline stage\n        titles: List of titles for each stage\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    n_stages = len(df_list)\n    fig, axes = plt.subplots(n_stages, 2, figsize=figsize)\n    \n    if n_stages == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, (df, title) in enumerate(zip(df_list, titles)):\n        # Left: Gaze trajectory\n        ax = axes[i, 0]\n        valid = df[::10].dropna(subset=['x_left', 'y_left', 'x_right', 'y_right'])\n        ax.scatter(valid['x_left'], valid['y_left'], c='blue', alpha=0.3, s=1, label='Left')\n        ax.scatter(valid['x_right'], valid['y_right'], c='red', alpha=0.3, s=1, label='Right')\n        ax.set_xlim(-200, SCREEN_WIDTH + 200)\n        ax.set_ylim(-200, SCREEN_HEIGHT + 200)\n        ax.invert_yaxis()\n        ax.set_title(f'{title} - Gaze Trajectory')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # Right: Validity over time\n        ax = axes[i, 1]\n        window_size = 500\n        validity = []\n        time_points = []\n        \n        for j in range(0, len(df) - window_size, window_size // 2):\n            window = df.iloc[j:j+window_size]\n            valid_pct = window['both_eyes_valid'].sum() / len(window) * 100\n            validity.append(valid_pct)\n            time_points.append(window['time_seconds'].mean())\n        \n        ax.plot(time_points, validity, 'g-', linewidth=2)\n        ax.axhline(y=80, color='orange', linestyle='--', alpha=0.5)\n        ax.set_ylim(0, 105)\n        ax.set_xlabel('Time (seconds)')\n        ax.set_ylabel('Data Validity (%)')\n        ax.set_title(f'{title} - Validity Over Time')\n        ax.grid(True, alpha=0.3)\n    \n    plt.suptitle('Eye-Tracking Data Through Preprocessing Pipeline', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n\ndef plot_blink_analysis(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (15, 8)\n) -> plt.Figure:\n    \"\"\"\n    Analyze and visualize blink patterns.\n    \n    Args:\n        df: Dataframe with blink data\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=figsize)\n    \n    # 1. Blink timeline\n    ax = axes[0, 0]\n    blink_left = df[df['is_blink_left']]\n    blink_right = df[df['is_blink_right']]\n    \n    ax.scatter(blink_left['time_seconds'], np.ones(len(blink_left)), \n               color='blue', s=20, alpha=0.6, label='Left eye blinks')\n    ax.scatter(blink_right['time_seconds'], np.ones(len(blink_right))*1.1, \n               color='red', s=20, alpha=0.6, label='Right eye blinks')\n    \n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylim(0.5, 1.5)\n    ax.set_yticks([])\n    ax.set_title('Blink Timeline')\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    # 2. Blink frequency over time\n    ax = axes[0, 1]\n    window_size = 5000  # 10-second windows\n    blink_freq_left = []\n    blink_freq_right = []\n    time_windows = []\n    \n    for i in range(0, len(df) - window_size, window_size):\n        window = df.iloc[i:i+window_size]\n        freq_left = window['is_blink_left'].sum() / (window_size / SAMPLING_RATE)\n        freq_right = window['is_blink_right'].sum() / (window_size / SAMPLING_RATE)\n        blink_freq_left.append(freq_left)\n        blink_freq_right.append(freq_right)\n        time_windows.append(window['time_seconds'].mean())\n    \n    ax.plot(time_windows, blink_freq_left, 'b-', label='Left eye', linewidth=2)\n    ax.plot(time_windows, blink_freq_right, 'r-', label='Right eye', linewidth=2)\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Blinks per second')\n    ax.set_title('Blink Frequency Over Time (10s windows)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 3. Missing data periods\n    ax = axes[1, 0]\n    missing_left = df['x_left'].isna() | df['y_left'].isna()\n    missing_right = df['x_right'].isna() | df['y_right'].isna()\n    \n    # Plot missing data as horizontal bars\n    y_pos = 0\n    for eye, missing in [('Left', missing_left), ('Right', missing_right)]:\n        # Find continuous missing periods\n        diff = missing.astype(int).diff()\n        starts = df.loc[diff == 1, 'time_seconds'].values\n        ends = df.loc[diff == -1, 'time_seconds'].values\n        \n        # Handle edge cases\n        if missing.iloc[0]:\n            starts = np.concatenate([[df['time_seconds'].iloc[0]], starts])\n        if missing.iloc[-1]:\n            ends = np.concatenate([ends, [df['time_seconds'].iloc[-1]]])\n        \n        # Plot bars\n        for start, end in zip(starts, ends):\n            ax.barh(y_pos, end - start, left=start, height=0.8, \n                    color='blue' if eye == 'Left' else 'red', alpha=0.5)\n        y_pos += 1\n    \n    ax.set_ylim(-0.5, 1.5)\n    ax.set_yticks([0, 1])\n    ax.set_yticklabels(['Left', 'Right'])\n    ax.set_xlabel('Time (seconds)')\n    ax.set_title('Missing Data Periods')\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    # 4. Binocular coordination\n    ax = axes[1, 1]\n    both_blinks = df['is_blink_left'] & df['is_blink_right']\n    left_only = df['is_blink_left'] & ~df['is_blink_right']\n    right_only = ~df['is_blink_left'] & df['is_blink_right']\n    \n    categories = ['Both eyes', 'Left only', 'Right only']\n    counts = [both_blinks.sum(), left_only.sum(), right_only.sum()]\n    \n    bars = ax.bar(categories, counts, color=['purple', 'blue', 'red'])\n    ax.set_ylabel('Number of samples')\n    ax.set_title('Blink Coordination')\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels on bars\n    for bar, count in zip(bars, counts):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{count}', ha='center', va='bottom')\n    \n    plt.suptitle('Blink Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n\ndef plot_saccade_analysis(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (15, 10)\n) -> plt.Figure:\n    \"\"\"\n    Extract and analyze saccade patterns.\n    \n    Args:\n        df: Dataframe with saccade data\n        figsize: Figure size\n        \n    Returns:\n        fig: Matplotlib figure\n    \"\"\"\n    # Extract saccade events\n    saccade_events = []\n    in_saccade = False\n    \n    for idx, row in df.iterrows():\n        both_saccade = row['is_saccade_left'] and row['is_saccade_right']\n        \n        if both_saccade and not in_saccade:\n            in_saccade = True\n            start_idx = idx\n            start_x_left = row['x_left']\n            start_y_left = row['y_left']\n            start_time = row['time_seconds']\n            \n        elif not both_saccade and in_saccade:\n            if pd.notna(start_x_left) and pd.notna(row['x_left']):\n                distance = np.sqrt((row['x_left'] - start_x_left)**2 + \n                                 (row['y_left'] - start_y_left)**2)\n                duration = row['time_seconds'] - start_time\n                \n                saccade_events.append({\n                    'start_time': start_time,\n                    'duration_ms': duration * 1000,\n                    'amplitude': distance,\n                    'start_x': start_x_left,\n                    'start_y': start_y_left,\n                    'end_x': row['x_left'],\n                    'end_y': row['y_left']\n                })\n            in_saccade = False\n    \n    saccades_df = pd.DataFrame(saccade_events)\n    \n    # Create plots\n    fig, axes = plt.subplots(2, 2, figsize=figsize)\n    \n    # 1. Saccade amplitudes over time\n    ax = axes[0, 0]\n    if len(saccades_df) > 0:\n        ax.scatter(saccades_df['start_time'], saccades_df['amplitude'],\n                   alpha=0.6, s=20, c='darkblue')\n        ax.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='Small')\n        ax.axhline(y=500, color='orange', linestyle='--', alpha=0.5, label='Medium')\n        ax.axhline(y=1000, color='red', linestyle='--', alpha=0.5, label='Large')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Saccade Amplitude (pixels)')\n    ax.set_title('Saccade Amplitudes Throughout Recording')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 2. Saccade duration vs amplitude\n    ax = axes[0, 1]\n    if len(saccades_df) > 0:\n        ax.scatter(saccades_df['duration_ms'], saccades_df['amplitude'],\n                   alpha=0.6, s=20)\n        ax.set_xlabel('Duration (ms)')\n        ax.set_ylabel('Amplitude (pixels)')\n        ax.set_title('Saccade Main Sequence')\n    ax.grid(True, alpha=0.3)\n    \n    # 3. Saccade directions (vector plot)\n    ax = axes[1, 0]\n    if len(saccades_df) > 0:\n        # Sample saccades to avoid overcrowding\n        sample_idx = np.random.choice(len(saccades_df), \n                                    min(100, len(saccades_df)), \n                                    replace=False)\n        for idx in sample_idx:\n            sac = saccades_df.iloc[idx]\n            dx = sac['end_x'] - sac['start_x']\n            dy = sac['end_y'] - sac['start_y']\n            ax.arrow(sac['start_x'], sac['start_y'], dx, dy,\n                    head_width=20, head_length=30, fc='blue', \n                    ec='blue', alpha=0.5, length_includes_head=True)\n    \n    screen_rect = Rectangle((0, 0), SCREEN_WIDTH, SCREEN_HEIGHT,\n                           fill=False, edgecolor='black', linewidth=2)\n    ax.add_patch(screen_rect)\n    ax.set_xlim(-100, SCREEN_WIDTH + 100)\n    ax.set_ylim(-100, SCREEN_HEIGHT + 100)\n    ax.invert_yaxis()\n    ax.set_xlabel('X coordinate (pixels)')\n    ax.set_ylabel('Y coordinate (pixels)')\n    ax.set_title(f'Saccade Vectors (n={min(100, len(saccades_df))} sampled)')\n    ax.set_aspect('equal')\n    \n    # 4. Saccade statistics\n    ax = axes[1, 1]\n    if len(saccades_df) > 0:\n        stats_text = f\"Total saccades: {len(saccades_df)}\\n\\n\"\n        stats_text += f\"Amplitude (pixels):\\n\"\n        stats_text += f\"  Mean: {saccades_df['amplitude'].mean():.1f}\\n\"\n        stats_text += f\"  Median: {saccades_df['amplitude'].median():.1f}\\n\"\n        stats_text += f\"  Std: {saccades_df['amplitude'].std():.1f}\\n\\n\"\n        stats_text += f\"Duration (ms):\\n\"\n        stats_text += f\"  Mean: {saccades_df['duration_ms'].mean():.1f}\\n\"\n        stats_text += f\"  Median: {saccades_df['duration_ms'].median():.1f}\\n\"\n        stats_text += f\"  Std: {saccades_df['duration_ms'].std():.1f}\\n\\n\"\n        stats_text += f\"Rate: {len(saccades_df) / df['time_seconds'].max():.2f} saccades/s\"\n        \n        ax.text(0.1, 0.9, stats_text, transform=ax.transAxes,\n                verticalalignment='top', fontsize=12, family='monospace')\n    ax.axis('off')\n    ax.set_title('Saccade Statistics')\n    \n    plt.suptitle(f'Binocular Saccade Analysis (n={len(saccades_df)} saccades)', \n                 fontsize=16)\n    plt.tight_layout()\n    \n    return fig",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.598068Z",
     "start_time": "2025-05-26T21:27:54.512945Z"
    }
   },
   "id": "2e907e8315907fbb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Complete Pipeline\n\nThe main preprocessing pipeline that combines all steps:",
   "metadata": {},
   "id": "63007ee7b504a432"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.629590Z",
     "start_time": "2025-05-26T21:27:54.616692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_eye_stabilization_effect(\n",
    "    df_before: pd.DataFrame,\n",
    "    df_after: pd.DataFrame,\n",
    "    time_window: Tuple[float, float] = (30, 35),\n",
    "    figsize: Tuple[int, int] = (15, 10)\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize the effect of eye stabilization on bouncy data.\n",
    "\n",
    "    Args:\n",
    "        df_before: DataFrame before stabilization\n",
    "        df_after: DataFrame after stabilization\n",
    "        time_window: Time window to zoom in on (seconds)\n",
    "        figsize: Figure size\n",
    "\n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "\n",
    "    # Time mask for zoomed view\n",
    "    mask = (df_before['time_seconds'] >= time_window[0]) & \\\n",
    "           (df_before['time_seconds'] <= time_window[1])\n",
    "\n",
    "    # 1. X coordinates before/after\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'x_left'],\n",
    "            'b-', alpha=0.5, linewidth=1, label='Left before')\n",
    "    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'x_right'],\n",
    "            'r-', alpha=0.5, linewidth=1, label='Right before')\n",
    "    ax.plot(df_after.loc[mask, 'time_seconds'], df_after.loc[mask, 'x_left'],\n",
    "            'b-', alpha=1, linewidth=2, label='Left after')\n",
    "    ax.plot(df_after.loc[mask, 'time_seconds'], df_after.loc[mask, 'x_right'],\n",
    "            'r-', alpha=1, linewidth=2, label='Right after')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('X coordinate (pixels)')\n",
    "    ax.set_title(f'Horizontal Eye Position ({time_window[0]}-{time_window[1]}s)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Y coordinates before/after\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'y_left'],\n",
    "            'b-', alpha=0.5, linewidth=1, label='Left before')\n",
    "    ax.plot(df_before.loc[mask, 'time_seconds'], df_before.loc[mask, 'y_right'],\n",
    "            'r-', alpha=0.5, linewidth=1, label='Right before')\n",
    "    ax.plot(df_after.loc[mask, 'time_seconds'], df_after.loc[mask, 'y_left'],\n",
    "            'b-', alpha=1, linewidth=2, label='Left after')\n",
    "    ax.plot(df_after.loc[mask, 'time_seconds'], df_after.loc[mask, 'y_right'],\n",
    "            'r-', alpha=1, linewidth=2, label='Right after')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Y coordinate (pixels)')\n",
    "    ax.set_title(f'Vertical Eye Position ({time_window[0]}-{time_window[1]}s)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Noise analysis\n",
    "    ax = axes[1, 0]\n",
    "    # Calculate noise (second derivative) in sliding windows\n",
    "    window_size = 50\n",
    "    noise_left_before = df_before['x_left'].diff().diff().rolling(window_size).std()\n",
    "    noise_right_before = df_before['x_right'].diff().diff().rolling(window_size).std()\n",
    "    noise_left_after = df_after['x_left'].diff().diff().rolling(window_size).std()\n",
    "    noise_right_after = df_after['x_right'].diff().diff().rolling(window_size).std()\n",
    "\n",
    "    ax.plot(df_before['time_seconds'], noise_left_before, 'b-', alpha=0.5, label='Left before')\n",
    "    ax.plot(df_before['time_seconds'], noise_right_before, 'r-', alpha=0.5, label='Right before')\n",
    "    ax.plot(df_after['time_seconds'], noise_left_after, 'b-', linewidth=2, label='Left after')\n",
    "    ax.plot(df_after['time_seconds'], noise_right_after, 'r-', linewidth=2, label='Right after')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Movement Noise (pixels)')\n",
    "    ax.set_title('Eye Movement Noise Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "    # 4. Eye position relationship\n",
    "    ax = axes[1, 1]\n",
    "    valid_before = df_before[df_before['both_eyes_valid']]\n",
    "    valid_after = df_after[df_after['both_eyes_valid']]\n",
    "\n",
    "    # Plot eye relationship\n",
    "    ax.scatter(valid_before['x_left'], valid_before['x_right'],\n",
    "               alpha=0.3, s=1, c='gray', label='Before')\n",
    "    ax.scatter(valid_after['x_left'], valid_after['x_right'],\n",
    "               alpha=0.5, s=1, c='green', label='After')\n",
    "\n",
    "    # Add diagonal line (perfect alignment)\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.plot(xlim, xlim, 'k--', alpha=0.5, label='Perfect alignment')\n",
    "\n",
    "    # Add anatomical constraint line\n",
    "    ax.plot(xlim, [x + 40 for x in xlim], 'r--', alpha=0.5,\n",
    "            label='Min eye distance (40px)')\n",
    "\n",
    "    ax.set_xlabel('Left Eye X (pixels)')\n",
    "    ax.set_ylabel('Right Eye X (pixels)')\n",
    "    ax.set_title('Eye Position Relationship')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.suptitle('Eye Stabilization and Anatomical Constraint Effects', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ],
   "id": "8a99269671c41a41",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "51d748077ffc1b15"
  },
  {
   "cell_type": "code",
   "source": "# [This cell removed - duplicate function definition]",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.912832Z",
     "start_time": "2025-05-26T21:27:54.911243Z"
    }
   },
   "id": "8c6dc50df0743303",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# [This cell removed - duplicate function definition]",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:54.937161Z",
     "start_time": "2025-05-26T21:27:54.935630Z"
    }
   },
   "id": "d3ecd93112b9e4a3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:55.000852Z",
     "start_time": "2025-05-26T21:27:54.992890Z"
    }
   },
   "cell_type": "code",
   "source": "def preprocess_eye_tracking_data(\n    file_path: str,\n    verbose: bool = True,\n    create_plots: bool = False\n) -> Tuple[pd.DataFrame, Dict]:\n\n    if verbose:\n        print(f\"\\n🚀 Complete Preprocessing Pipeline\")\n        print(f\"File: {os.path.basename(file_path)}\")\n        print(\"=\"*60)\n\n    # Step 1: Load data\n    df = pd.read_csv(file_path)\n    initial_samples = len(df)\n    subject_id = os.path.basename(file_path).split('_')[0]\n\n    # Save original for comparison\n    df_original = df.copy()\n\n    # Remove empty columns\n    df = remove_empty_columns(df, threshold=PREPROCESSING_PARAMS['missing_data_threshold'])\n\n    if verbose:\n        print(f\"✓ Step 1: Loaded data ({initial_samples:,} samples)\")\n\n    # Step 2: Add helper features\n    df = add_helper_features(df)\n    df_original = add_helper_features(df_original)  # Also add to original for plotting\n    if verbose:\n        print(\"✓ Step 2: Added helper features\")\n\n    # Save intermediate stages for visualization\n    pipeline_stages = []\n    pipeline_titles = []\n\n    # Step 3: Detect and fix eye swap if needed\n    df = detect_and_fix_eye_swap(df, verbose=verbose)\n    if verbose:\n        print(\"✓ Step 3: Checked gaze labeling\")\n    \n    # Save before stabilization for comparison\n    df_before_stabilization = df.copy()\n    \n    # Step 4: Stabilize bouncy eye\n    df = stabilize_bouncy_eye(df, verbose=verbose)\n    if verbose:\n        print(\"✓ Step 4: Stabilized eye movements\")\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Stabilization\")\n\n    # Step 5: Detect and handle blinks\n    df, blink_stats = detect_and_handle_blinks(df, verbose=verbose)\n    after_blinks = len(df)\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Blink Handling\")\n\n    # Step 6: Enforce binocular validity\n    df = enforce_binocular_validity(df)\n    binocular_samples = df['both_eyes_valid'].sum()\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Binocular Enforcement\")\n\n    # Step 7: Apply calibration correction\n    df, correction_info = automatic_binocular_calibration(df, verbose=verbose)\n    after_calibration = len(df)\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Calibration\")\n    \n    # Step 8: Enforce gaze quality constraints (UPDATED)\n    df = enforce_gaze_quality_constraints(df, verbose=verbose)\n    after_gaze_quality = df['both_eyes_valid'].sum()\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Gaze Quality Filtering\")\n\n    # Step 9: Remove extreme disparities (keep existing for additional filtering)\n    df = remove_extreme_disparities(df)\n    valid_after_disparity = df['both_eyes_valid'].sum()\n    pipeline_stages.append(df.copy())\n    pipeline_titles.append(\"After Disparity Filtering\")\n\n    # Step 10: Round coordinates to pixels\n    df = round_coordinates_to_pixels(df, verbose=verbose)\n\n    # Step 11: Preprocess pupil size\n    df, pupil_stats = preprocess_pupil_size(df, verbose=verbose)\n\n    # Calculate final statistics\n    final_valid_samples = df['both_eyes_valid'].sum()\n\n    preprocessing_info = {\n        'subject_id': subject_id,\n        'file': os.path.basename(file_path),\n        'initial_samples': initial_samples,\n        'final_valid_samples': final_valid_samples,\n        'retention_rate': final_valid_samples / initial_samples * 100,\n        'blink_stats': blink_stats,\n        'calibration_correction': correction_info,\n        'pupil_stats': pupil_stats,\n        'steps': {\n            'after_blinks': after_blinks,\n            'after_binocular': binocular_samples,\n            'after_calibration': after_calibration,\n            'after_gaze_quality': after_gaze_quality,\n            'after_disparity': valid_after_disparity,\n            'final': final_valid_samples\n        }\n    }\n\n    if verbose:\n        print(\"\\n\" + \"=\"*60)\n        print(f\"📊 Final Summary:\")\n        print(f\"   Initial samples: {initial_samples:,}\")\n        print(f\"   Final valid samples: {final_valid_samples:,}\")\n        print(f\"   Data retention: {preprocessing_info['retention_rate']:.1f}%\")\n        print(\"=\"*60)\n\n    # Create plots if requested\n    if create_plots:\n        # Create output directory for subject\n        subject_dir = os.path.join(FIGURE_DIR, subject_id)\n        os.makedirs(subject_dir, exist_ok=True)\n\n        # 1. Gaze trajectory comparison\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n        plot_gaze_trajectory(df_original, ax=ax1, title='Before Preprocessing')\n        plot_gaze_trajectory(df, ax=ax2, title='After Preprocessing')\n        plt.suptitle(f'Gaze Trajectory Comparison - Subject {subject_id}')\n        plt.tight_layout()\n        plt.savefig(os.path.join(subject_dir, 'gaze_trajectory_comparison.png'))\n        plt.close()\n\n        # 2. Disparity analysis\n        fig = plot_disparity_analysis(df)\n        plt.savefig(os.path.join(subject_dir, 'disparity_analysis.png'))\n        plt.close()\n\n        # 3. Preprocessing summary\n        fig = plot_preprocessing_summary(preprocessing_info)\n        plt.savefig(os.path.join(subject_dir, 'preprocessing_summary.png'))\n        plt.close()\n\n        # 4. Pupil analysis\n        fig = plot_pupil_analysis(df_original, df)\n        plt.savefig(os.path.join(subject_dir, 'pupil_analysis.png'))\n        plt.close()\n\n        # 5. Pipeline visualization\n        fig = plot_preprocessing_pipeline(pipeline_stages, pipeline_titles)\n        plt.savefig(os.path.join(subject_dir, 'preprocessing_pipeline.png'))\n        plt.close()\n\n        # 6. Blink analysis\n        fig = plot_blink_analysis(df)\n        plt.savefig(os.path.join(subject_dir, 'blink_analysis.png'))\n        plt.close()\n\n        # 7. Saccade analysis\n        fig = plot_saccade_analysis(df)\n        plt.savefig(os.path.join(subject_dir, 'saccade_analysis.png'))\n        plt.close()\n        \n        # 8. Eye stabilization effect\n        fig = plot_eye_stabilization_effect(df_before_stabilization, df)\n        plt.savefig(os.path.join(subject_dir, 'eye_stabilization_effect.png'))\n        plt.close()\n\n        if verbose:\n            print(f\"\\n📊 Plots saved to: {subject_dir}\")\n            print(\"   - gaze_trajectory_comparison.png\")\n            print(\"   - disparity_analysis.png\")\n            print(\"   - preprocessing_summary.png\")\n            print(\"   - pupil_analysis.png\")\n            print(\"   - preprocessing_pipeline.png\")\n            print(\"   - blink_analysis.png\")\n            print(\"   - saccade_analysis.png\")\n            print(\"   - eye_stabilization_effect.png\")\n\n    return df, preprocessing_info",
   "id": "22479fd875d4d4fa",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "def batch_preprocess_files(\n    file_pattern: str = None,\n    output_dir: str = None,\n    create_plots: bool = False,\n    save_preprocessed: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Batch process multiple eye-tracking files.\n    \n    Args:\n        file_pattern: Glob pattern for files (default: all CSVs in DATA_DIR)\n        output_dir: Directory to save processed files (default: OUTPUT_DIR)\n        create_plots: Generate plots for each file\n        save_preprocessed: Save preprocessed data to CSV\n        \n    Returns:\n        summary_df: DataFrame with preprocessing summary for all files\n    \"\"\"\n    if file_pattern is None:\n        file_pattern = os.path.join(DATA_DIR, '*.csv')\n    if output_dir is None:\n        output_dir = OUTPUT_DIR\n    \n    # Get all files\n    files = sorted(glob.glob(file_pattern))\n    print(f\"Found {len(files)} files to process\")\n    \n    # Process each file\n    results = []\n    \n    for i, file_path in enumerate(files):\n        print(f\"\\n{'='*60}\")\n        print(f\"Processing file {i+1}/{len(files)}\")\n        \n        try:\n            # Process file\n            df_processed, info = preprocess_eye_tracking_data(\n                file_path,\n                verbose=True,\n                create_plots=create_plots\n            )\n            \n            # Save processed data\n            if save_preprocessed:\n                output_file = os.path.join(\n                    output_dir,\n                    f\"{info['subject_id']}_preprocessed.csv\"\n                )\n                df_processed.to_csv(output_file, index=False)\n                print(f\"✓ Saved to: {output_file}\")\n            \n            # Add to results\n            results.append({\n                'subject_id': info['subject_id'],\n                'file': info['file'],\n                'initial_samples': info['initial_samples'],\n                'final_samples': info['final_valid_samples'],\n                'retention_rate': info['retention_rate'],\n                'blinks_left': info['blink_stats']['left_eye']['natural_blinks'],\n                'blinks_right': info['blink_stats']['right_eye']['natural_blinks'],\n                'calibration_improvement': info['calibration_correction']['improvement_percent'],\n                'pupil_removed_left': info['pupil_stats'].get('left_eye', {}).get('removal_percentage', 0),\n                'pupil_removed_right': info['pupil_stats'].get('right_eye', {}).get('removal_percentage', 0),\n            })\n            \n        except Exception as e:\n            print(f\"❌ Error processing {file_path}: {str(e)}\")\n            results.append({\n                'subject_id': os.path.basename(file_path).split('_')[0],\n                'file': os.path.basename(file_path),\n                'error': str(e)\n            })\n    \n    # Create summary dataframe\n    summary_df = pd.DataFrame(results)\n    \n    # Save summary\n    summary_file = os.path.join(output_dir, 'preprocessing_summary.csv')\n    summary_df.to_csv(summary_file, index=False)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"✅ Batch processing complete!\")\n    print(f\"   Processed: {len(files)} files\")\n    print(f\"   Average retention: {summary_df['retention_rate'].mean():.1f}%\")\n    print(f\"   Summary saved to: {summary_file}\")\n    \n    return summary_df\n\n\ndef analyze_batch_results(summary_df: pd.DataFrame) -> plt.Figure:\n    \"\"\"\n    Analyze and visualize batch preprocessing results.\n    \n    Args:\n        summary_df: DataFrame with batch results\n        \n    Returns:\n        fig: Summary visualization\n    \"\"\"\n    # Remove error rows for analysis\n    valid_df = summary_df[~summary_df.get('error', pd.Series([False]*len(summary_df)))]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. Retention rates distribution\n    ax = axes[0, 0]\n    ax.hist(valid_df['retention_rate'], bins=20, edgecolor='black', alpha=0.7)\n    ax.axvline(valid_df['retention_rate'].mean(), color='red', linestyle='--',\n               label=f'Mean: {valid_df[\"retention_rate\"].mean():.1f}%')\n    ax.axvline(ANALYSIS_PARAMS['min_valid_data'] * 100, color='orange', linestyle='--',\n               label=f'Minimum: {ANALYSIS_PARAMS[\"min_valid_data\"] * 100:.0f}%')\n    ax.set_xlabel('Data Retention Rate (%)')\n    ax.set_ylabel('Number of Files')\n    ax.set_title('Data Retention Distribution')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 2. Blink statistics\n    ax = axes[0, 1]\n    blink_data = pd.DataFrame({\n        'Left': valid_df['blinks_left'],\n        'Right': valid_df['blinks_right']\n    })\n    blink_data.plot(kind='box', ax=ax)\n    ax.set_ylabel('Number of Blinks')\n    ax.set_title('Blink Detection Statistics')\n    ax.grid(True, alpha=0.3)\n    \n    # 3. Calibration improvements\n    ax = axes[1, 0]\n    improvements = valid_df['calibration_improvement'].dropna()\n    ax.hist(improvements, bins=20, edgecolor='black', alpha=0.7)\n    ax.axvline(improvements.mean(), color='red', linestyle='--',\n               label=f'Mean: {improvements.mean():.1f}%')\n    ax.set_xlabel('Calibration Improvement (%)')\n    ax.set_ylabel('Number of Files')\n    ax.set_title('Binocular Calibration Improvements')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 4. Subject comparison\n    ax = axes[1, 1]\n    sorted_df = valid_df.sort_values('retention_rate')\n    y_pos = np.arange(len(sorted_df))\n    ax.barh(y_pos, sorted_df['retention_rate'])\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(sorted_df['subject_id'], fontsize=8)\n    ax.set_xlabel('Data Retention Rate (%)')\n    ax.set_title('Retention Rate by Subject')\n    ax.axvline(ANALYSIS_PARAMS['min_valid_data'] * 100, color='orange', linestyle='--')\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    plt.suptitle('Batch Preprocessing Results Summary', fontsize=16)\n    plt.tight_layout()\n    \n    return fig",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:55.047766Z",
     "start_time": "2025-05-26T21:27:55.039907Z"
    }
   },
   "id": "262cb328e99675ee",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Feature Extraction\n\nExtract features from preprocessed data for machine learning:",
   "metadata": {},
   "id": "4e689bb4dd015803"
  },
  {
   "cell_type": "code",
   "source": "def extract_basic_features(df: pd.DataFrame) -> Dict[str, float]:\n    \"\"\"\n    Extract basic features from preprocessed eye-tracking data.\n    \n    Args:\n        df: Preprocessed dataframe\n        \n    Returns:\n        features: Dictionary of extracted features\n    \"\"\"\n    features = {}\n    \n    # Data quality features\n    features['valid_samples_ratio'] = df['both_eyes_valid'].sum() / len(df)\n    features['total_duration_seconds'] = df['time_seconds'].max()\n    \n    # Fixation features\n    features['fixation_ratio_left'] = df['is_fixation_left'].sum() / len(df)\n    features['fixation_ratio_right'] = df['is_fixation_right'].sum() / len(df)\n    features['fixation_ratio_binocular'] = (\n        (df['is_fixation_left'] & df['is_fixation_right']).sum() / len(df)\n    )\n    \n    # Saccade features\n    features['saccade_ratio_left'] = df['is_saccade_left'].sum() / len(df)\n    features['saccade_ratio_right'] = df['is_saccade_right'].sum() / len(df)\n    features['saccade_ratio_binocular'] = (\n        (df['is_saccade_left'] & df['is_saccade_right']).sum() / len(df)\n    )\n    \n    # Blink features\n    features['blink_count_left'] = df['is_blink_left'].sum() / SAMPLING_RATE  # per second\n    features['blink_count_right'] = df['is_blink_right'].sum() / SAMPLING_RATE\n    \n    # Disparity features (only for valid samples)\n    valid_mask = df['both_eyes_valid']\n    if valid_mask.sum() > 0:\n        features['disparity_mean'] = df.loc[valid_mask, 'disparity_total'].mean()\n        features['disparity_std'] = df.loc[valid_mask, 'disparity_total'].std()\n        features['disparity_median'] = df.loc[valid_mask, 'disparity_total'].median()\n        features['disparity_95percentile'] = df.loc[valid_mask, 'disparity_total'].quantile(0.95)\n    \n    # Velocity features\n    for eye in ['left', 'right']:\n        vel_col = f'velocity_{eye}'\n        if vel_col in df.columns:\n            valid_vel = df[vel_col].dropna()\n            if len(valid_vel) > 0:\n                features[f'velocity_mean_{eye}'] = valid_vel.mean()\n                features[f'velocity_std_{eye}'] = valid_vel.std()\n                features[f'velocity_median_{eye}'] = valid_vel.median()\n    \n    # Pupil features\n    for eye in ['left', 'right']:\n        pupil_col = f'pupil_{eye}'\n        if pupil_col in df.columns:\n            valid_pupil = df[pupil_col].dropna()\n            if len(valid_pupil) > 0:\n                features[f'pupil_mean_{eye}'] = valid_pupil.mean()\n                features[f'pupil_std_{eye}'] = valid_pupil.std()\n                features[f'pupil_cv_{eye}'] = valid_pupil.std() / valid_pupil.mean()\n    \n    # Spatial distribution features\n    for eye in ['left', 'right']:\n        x_col, y_col = f'x_{eye}', f'y_{eye}'\n        valid_data = df[[x_col, y_col]].dropna()\n        \n        if len(valid_data) > 0:\n            # Coverage of screen\n            x_range = valid_data[x_col].max() - valid_data[x_col].min()\n            y_range = valid_data[y_col].max() - valid_data[y_col].min()\n            features[f'screen_coverage_{eye}'] = (x_range * y_range) / (SCREEN_WIDTH * SCREEN_HEIGHT)\n            \n            # Center of mass\n            features[f'center_x_{eye}'] = valid_data[x_col].mean()\n            features[f'center_y_{eye}'] = valid_data[y_col].mean()\n            \n            # Spread (standard deviation)\n            features[f'spread_x_{eye}'] = valid_data[x_col].std()\n            features[f'spread_y_{eye}'] = valid_data[y_col].std()\n    \n    return features\n\n\ndef extract_temporal_features(\n    df: pd.DataFrame,\n    window_size: int = None\n) -> pd.DataFrame:\n    \"\"\"\n    Extract temporal features using sliding windows.\n    \n    Args:\n        df: Preprocessed dataframe\n        window_size: Window size in samples (default from config)\n        \n    Returns:\n        temporal_features: DataFrame with temporal features\n    \"\"\"\n    if window_size is None:\n        window_size = ANALYSIS_PARAMS['window_size']\n    \n    temporal_features = []\n    \n    # Slide window through data\n    for i in range(0, len(df) - window_size, window_size // 2):\n        window = df.iloc[i:i+window_size]\n        \n        # Extract features for this window\n        window_features = {\n            'window_start': i,\n            'window_end': i + window_size,\n            'time_start': window['time_seconds'].iloc[0],\n            'time_end': window['time_seconds'].iloc[-1],\n        }\n        \n        # Add basic features for this window\n        basic_features = extract_basic_features(window)\n        window_features.update(basic_features)\n        \n        temporal_features.append(window_features)\n    \n    return pd.DataFrame(temporal_features)\n\n\ndef create_feature_matrix(\n    preprocessed_files: List[str],\n    labels: Optional[Dict[str, int]] = None\n) -> Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create feature matrix from multiple preprocessed files.\n    \n    Args:\n        preprocessed_files: List of preprocessed CSV files\n        labels: Optional dictionary mapping subject_id to label\n        \n    Returns:\n        X: Feature matrix\n        y: Labels (if provided)\n    \"\"\"\n    all_features = []\n    all_labels = []\n    \n    for file_path in preprocessed_files:\n        # Load preprocessed data\n        df = pd.read_csv(file_path)\n        \n        # Extract subject ID\n        subject_id = os.path.basename(file_path).split('_')[0]\n        \n        # Extract features\n        features = extract_basic_features(df)\n        features['subject_id'] = subject_id\n        \n        all_features.append(features)\n        \n        # Add label if available\n        if labels and subject_id in labels:\n            all_labels.append(labels[subject_id])\n    \n    # Create feature matrix\n    X = pd.DataFrame(all_features)\n    X = X.set_index('subject_id')\n    \n    # Create label series\n    y = pd.Series(all_labels, index=X.index) if all_labels else None\n    \n    return X, y",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:55.088243Z",
     "start_time": "2025-05-26T21:27:55.080658Z"
    }
   },
   "id": "46374de2a9a77f7a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Export & Save\n\nHelper functions for saving and exporting processed data:",
   "metadata": {},
   "id": "8a020d800c0d4f1d"
  },
  {
   "cell_type": "code",
   "source": "def save_preprocessing_report(\n    preprocessing_info: Dict,\n    df_processed: pd.DataFrame,\n    output_dir: str = None\n) -> str:\n    \"\"\"\n    Generate and save a comprehensive preprocessing report.\n    \n    Args:\n        preprocessing_info: Dictionary with preprocessing statistics\n        df_processed: Processed dataframe\n        output_dir: Directory to save report (default: OUTPUT_DIR)\n        \n    Returns:\n        report_path: Path to saved report\n    \"\"\"\n    if output_dir is None:\n        output_dir = OUTPUT_DIR\n    \n    subject_id = preprocessing_info['subject_id']\n    report_path = os.path.join(output_dir, f\"{subject_id}_preprocessing_report.txt\")\n    \n    with open(report_path, 'w') as f:\n        f.write(\"EYE-TRACKING PREPROCESSING REPORT\\n\")\n        f.write(\"=\"*60 + \"\\n\\n\")\n        \n        # Basic information\n        f.write(f\"Subject ID: {subject_id}\\n\")\n        f.write(f\"File: {preprocessing_info['file']}\\n\")\n        f.write(f\"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(\"\\n\")\n        \n        # Data retention summary\n        f.write(\"DATA RETENTION SUMMARY\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        f.write(f\"Initial samples: {preprocessing_info['initial_samples']:,}\\n\")\n        f.write(f\"Final valid samples: {preprocessing_info['final_valid_samples']:,}\\n\")\n        f.write(f\"Retention rate: {preprocessing_info['retention_rate']:.1f}%\\n\")\n        f.write(\"\\n\")\n        \n        # Pipeline steps\n        f.write(\"PREPROCESSING PIPELINE\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        for step, samples in preprocessing_info['steps'].items():\n            f.write(f\"{step}: {samples:,} samples\\n\")\n        f.write(\"\\n\")\n        \n        # Blink statistics\n        f.write(\"BLINK DETECTION\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        for eye, stats in preprocessing_info['blink_stats'].items():\n            f.write(f\"{eye}:\\n\")\n            f.write(f\"  Natural blinks: {stats['natural_blinks']}\\n\")\n            f.write(f\"  Technical gaps: {stats['technical_gaps']}\\n\")\n            f.write(f\"  Interpolated samples: {stats['interpolated_samples']}\\n\")\n        f.write(\"\\n\")\n        \n        # Calibration correction\n        f.write(\"BINOCULAR CALIBRATION\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        cal = preprocessing_info['calibration_correction']\n        f.write(f\"Improvement: {cal['improvement_percent']:.1f}%\\n\")\n        f.write(f\"Original disparity: {cal.get('original_disparity', 0):.1f} pixels\\n\")\n        f.write(f\"Corrected disparity: {cal.get('corrected_disparity', 0):.1f} pixels\\n\")\n        f.write(\"\\n\")\n        \n        # Pupil preprocessing\n        f.write(\"PUPIL PREPROCESSING\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        for eye, stats in preprocessing_info['pupil_stats'].items():\n            f.write(f\"{eye}:\\n\")\n            f.write(f\"  Removed: {stats['removal_percentage']:.1f}%\\n\")\n            f.write(f\"  Final valid: {stats['final_valid']:,} samples\\n\")\n        f.write(\"\\n\")\n        \n        # Feature summary\n        f.write(\"FEATURE SUMMARY\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        features = extract_basic_features(df_processed)\n        for key, value in sorted(features.items()):\n            if isinstance(value, float):\n                f.write(f\"{key}: {value:.3f}\\n\")\n            else:\n                f.write(f\"{key}: {value}\\n\")\n    \n    print(f\"✅ Report saved to: {report_path}\")\n    return report_path\n\n\ndef export_for_matlab(\n    df: pd.DataFrame,\n    output_path: str,\n    include_features: bool = True\n) -> None:\n    \"\"\"\n    Export preprocessed data in MATLAB-friendly format.\n    \n    Args:\n        df: Preprocessed dataframe\n        output_path: Path for .mat file\n        include_features: Include extracted features\n    \"\"\"\n    from scipy.io import savemat\n    \n    # Prepare data dictionary\n    mat_data = {\n        'time_seconds': df['time_seconds'].values,\n        'x_left': df['x_left'].values,\n        'y_left': df['y_left'].values,\n        'x_right': df['x_right'].values,\n        'y_right': df['y_right'].values,\n        'pupil_left': df['pupil_left'].values,\n        'pupil_right': df['pupil_right'].values,\n        'is_fixation_left': df['is_fixation_left'].values.astype(int),\n        'is_fixation_right': df['is_fixation_right'].values.astype(int),\n        'is_saccade_left': df['is_saccade_left'].values.astype(int),\n        'is_saccade_right': df['is_saccade_right'].values.astype(int),\n        'is_blink_left': df['is_blink_left'].values.astype(int),\n        'is_blink_right': df['is_blink_right'].values.astype(int),\n        'both_eyes_valid': df['both_eyes_valid'].values.astype(int),\n    }\n    \n    # Add features if requested\n    if include_features:\n        features = extract_basic_features(df)\n        mat_data['features'] = features\n        mat_data['feature_names'] = list(features.keys())\n    \n    # Save to .mat file\n    savemat(output_path, mat_data)\n    print(f\"✅ Exported to MATLAB format: {output_path}\")\n\n\ndef create_preprocessing_module() -> None:\n    \"\"\"\n    Create a standalone Python module with all preprocessing functions.\n    \"\"\"\n    module_path = os.path.join(OUTPUT_DIR, 'asd_preprocessing.py')\n    \n    # Write the module\n    with open(module_path, 'w') as f:\n        f.write('\"\"\"\\nASD Eye-Tracking Preprocessing Module\\n')\n        f.write('Generated from asd_professional.ipynb\\n\"\"\"\\n\\n')\n        \n        # Add imports\n        f.write('import pandas as pd\\n')\n        f.write('import numpy as np\\n')\n        f.write('from typing import Dict, Tuple, List, Optional\\n')\n        f.write('from scipy.ndimage import gaussian_filter1d\\n\\n')\n        \n        # Add configuration\n        f.write('# Configuration\\n')\n        f.write(f'SCREEN_WIDTH = {SCREEN_WIDTH}\\n')\n        f.write(f'SCREEN_HEIGHT = {SCREEN_HEIGHT}\\n')\n        f.write(f'SAMPLING_RATE = {SAMPLING_RATE}\\n')\n        f.write(f'PREPROCESSING_PARAMS = {PREPROCESSING_PARAMS}\\n\\n')\n        \n        # Note about functions\n        f.write('# Note: Copy preprocessing functions from notebook\\n')\n        f.write('# Functions include:\\n')\n        f.write('# - add_helper_features()\\n')\n        f.write('# - remove_empty_columns()\\n')\n        f.write('# - enforce_binocular_validity()\\n')\n        f.write('# - automatic_binocular_calibration()\\n')\n        f.write('# - remove_extreme_disparities()\\n')\n        f.write('# - detect_and_handle_blinks()\\n')\n        f.write('# - preprocess_pupil_size()\\n')\n        f.write('# - round_coordinates_to_pixels()\\n')\n        f.write('# - preprocess_eye_tracking_data()\\n')\n        f.write('# - extract_basic_features()\\n')\n    \n    print(f\"✅ Module template created: {module_path}\")\n    print(\"   Copy preprocessing functions from notebook to complete the module\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:27:55.156227Z",
     "start_time": "2025-05-26T21:27:55.144401Z"
    }
   },
   "id": "9edcd5725d813184",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Example Usage\n\nComplete example demonstrating the full pipeline:",
   "metadata": {},
   "id": "15bfe6fbbdbbffc9"
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Process a single file\nif len(glob.glob(os.path.join(DATA_DIR, '*.csv'))) > 0:\n    # Get first available file\n    sample_file = glob.glob(os.path.join(DATA_DIR, '*.csv'))[0]\n    \n    # Process with visualization\n    df_processed, preprocessing_info = preprocess_eye_tracking_data(\n        sample_file,\n        verbose=True,\n        create_plots=True\n    )\n    \n    # Save processed data\n    output_file = os.path.join(OUTPUT_DIR, f\"{preprocessing_info['subject_id']}_processed.csv\")\n    df_processed.to_csv(output_file, index=False)\n    \n    # Generate report\n    report_path = save_preprocessing_report(preprocessing_info, df_processed)\n    \n    # Extract features\n    features = extract_basic_features(df_processed)\n    print(\"\\n📊 Extracted Features (sample):\")\n    for key, value in list(features.items())[:5]:\n        print(f\"   {key}: {value:.3f}\")\nelse:\n    print(\"⚠️ No CSV files found in data directory\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:28:01.401262Z",
     "start_time": "2025-05-26T21:27:55.191582Z"
    }
   },
   "id": "b7ce80b949d3b6d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Complete Preprocessing Pipeline\n",
      "File: 1024539343_unified_eye_metrics_Dinstein_Girls_90_SecX.csv\n",
      "============================================================\n",
      "Removing 2 columns with >95% missing data:\n",
      "  - input: 100.0% missing\n",
      "  - cr_info: 99.5% missing\n",
      "✓ Step 1: Loaded data (45,010 samples)\n",
      "✓ Step 2: Added helper features\n",
      "✓ Gaze label check: 50.8% of samples have right eye gaze to the right of left eye gaze\n",
      "   Mean horizontal gaze difference: 6.7px\n",
      "✓ Step 3: Checked gaze labeling\n",
      "✓ No significant bounce detected (L/R ratio: 0.75)\n",
      "✓ Step 4: Stabilized eye movements\n",
      "👁️ Blink Detection and Handling\n",
      "  Left eye: 3 blinks, 7 gaps\n",
      "  Right eye: 0 blinks, 3 gaps\n",
      "Found 560 monocular samples (1.2%)\n",
      "After enforcement: 44301 valid binocular samples (98.4%)\n",
      "🔧 Automatic Binocular Calibration\n",
      "  Detected bias: (61.0, 0.4) pixels\n",
      "  Based on 672 stable samples\n",
      "  Disparity reduced: 66.6 → 32.3 pixels\n",
      "  Improvement: 51.5%\n",
      "⚠️ Found 3343 samples with excessive gaze disparity (>100px)\n",
      "   (7.55% of valid data)\n",
      "   → Removing poor quality gaze samples\n",
      "\n",
      "📊 Gaze disparity statistics (remaining data):\n",
      "   Mean: 36.6px\n",
      "   Median: 34.1px\n",
      "   95th percentile: 76.8px\n",
      "\n",
      "✓ After gaze quality filtering: 40958 valid samples\n",
      "Found 0 samples with extreme disparity (>150 pixels)\n",
      "That's 0.00% of previously valid data\n",
      "After filtering: 40958 valid samples (91.0%)\n",
      "🎯 Coordinates rounded to integer pixels\n",
      "👁️ Pupil Size Preprocessing\n",
      "  Left eye: 29.4% removed (31,244 valid samples)\n",
      "  Right eye: 3.6% removed (42,716 valid samples)\n",
      "\n",
      "============================================================\n",
      "📊 Final Summary:\n",
      "   Initial samples: 45,010\n",
      "   Final valid samples: 40,958\n",
      "   Data retention: 91.0%\n",
      "============================================================\n",
      "\n",
      "📊 Plots saved to: figures/1024539343\n",
      "   - gaze_trajectory_comparison.png\n",
      "   - disparity_analysis.png\n",
      "   - preprocessing_summary.png\n",
      "   - pupil_analysis.png\n",
      "   - preprocessing_pipeline.png\n",
      "   - blink_analysis.png\n",
      "   - saccade_analysis.png\n",
      "   - eye_stabilization_effect.png\n",
      "✅ Report saved to: output/1024539343_preprocessing_report.txt\n",
      "\n",
      "📊 Extracted Features (sample):\n",
      "   valid_samples_ratio: 0.910\n",
      "   total_duration_seconds: 90.018\n",
      "   fixation_ratio_left: 0.880\n",
      "   fixation_ratio_right: 0.798\n",
      "   fixation_ratio_binocular: 0.783\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "# Example 2: Batch process all files\n# Uncomment to run batch processing\n# summary_df = batch_preprocess_files(\n#     create_plots=False,\n#     save_preprocessed=True\n# )\n# \n# # Analyze batch results\n# if len(summary_df) > 0:\n#     fig = analyze_batch_results(summary_df)\n#     plt.savefig(os.path.join(FIGURE_DIR, 'batch_analysis.png'))\n#     plt.show()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:28:01.418153Z",
     "start_time": "2025-05-26T21:28:01.416159Z"
    }
   },
   "id": "c8f75065f01ba7e2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# Example 3: Create feature matrix for machine learning\n# Uncomment to create feature matrix\n# preprocessed_files = glob.glob(os.path.join(OUTPUT_DIR, '*_processed.csv'))\n# \n# # Example labels (replace with actual labels)\n# labels = {\n#     '1017735502': 0,  # TD\n#     '1019729632': 1,  # ASD\n#     # Add more labels...\n# }\n# \n# X, y = create_feature_matrix(preprocessed_files, labels)\n# print(f\"Feature matrix shape: {X.shape}\")\n# print(f\"Features: {list(X.columns)[:10]}...\")  # Show first 10 features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:28:01.443003Z",
     "start_time": "2025-05-26T21:28:01.441160Z"
    }
   },
   "id": "1baf76ead4a967fb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis professional notebook provides a complete eye-tracking preprocessing pipeline for ASD research, including:\n\n✅ **Robust Preprocessing**\n- Binocular validity enforcement\n- Automatic calibration correction\n- Blink detection and interpolation\n- Outlier removal and smoothing\n\n✅ **Professional Tools**\n- Reusable visualization functions\n- Batch processing capabilities\n- Feature extraction for ML\n- Multiple export formats\n\n✅ **Quality Assurance**\n- Comprehensive error handling\n- Detailed preprocessing reports\n- Visual quality checks\n- Statistical summaries\n\n### Next Steps\n\n1. **Data Collection**: Process all available eye-tracking files\n2. **Feature Engineering**: Develop domain-specific features\n3. **Machine Learning**: Build classification models\n4. **Deep Learning**: Implement LSTM/CNN architectures\n5. **Validation**: Cross-validate on held-out data\n\n---\n\n**Remember**: This pipeline ensures high-quality, analysis-ready data for your autism research! 🚀",
   "metadata": {},
   "id": "b07cd45303f14ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:28:01.478253Z",
     "start_time": "2025-05-26T21:28:01.476790Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a8f6d3540cca7e81",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASD Analysis (venv)",
   "language": "python",
   "name": "asd-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
